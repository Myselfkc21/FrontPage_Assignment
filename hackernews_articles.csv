Title,Points,Author,Website,URL,Content
Proof and progress in mathematics (1994) [pdf],1 point,sonabinu,toronto.edu,https://www.math.toronto.edu/mccann/199/thurston.pdf,
Reviewing the OG Attention Paper,1 point,theahura,theahura.substack.com,https://theahura.substack.com/p/ilyas-30-papers-to-carmack-og-attention,"This post is part of a series of paper reviews, covering the ~30 papers Ilya Sutskever sent to John Carmack to learn about AI. To see the rest of the reviews, go here. | High level | Ah, the attention paper. It's hard to overstate how incredibly important this paper is to the ML canon. Attention has come up repeatedly in the papers we've reviewed thus far; here, we see its origins. | Back in 2015, folks were using LSTMs to model language. An LSTM would ingest words in a sequence one by one. For each word, the LSTM would update some embedding representation. And then some other sequence generator (often also an LSTM) would take that embedding representation and sequentially produce some output. | In the past I’ve emphasized the importance of embeddings and thinking about what embeddings actually represent. Architecture matters a lot here — the structure of a model has a direct impact on how data gets written to or erased from some set of embeddings. In the case of a standard seq2seq model, the LSTM is learning prefix embeddings. That is, each stage of the LSTM has to capture all of the data that came before it. | Funny enough, I wrote about this exact problem previously in the LSTM blog post review: | As the [RNN] processes each element in the sequence, it can learn a representation of the prefix (the data it has seen thus far) which can guide how it processes the next step. | … | RNNs in their most basic form — a single loop — have a big problem. The model doesn't ""know"" what information it might need at each future step. So, as a sequence gets longer, the model has to pack more and more information into the 'message' that is passed from one time step to the next. But it only has a fixed vector size with which to do so, and there is a limited amount of computation capacity in the single layer of weights that are used to update the state. At some point, the model runs into information theoretic limits. This in turn makes it hard to represent long range dependencies. | In that review, I discussed how LSTMs improve the ability to handle long-range dependencies. But it's not a permanent fix, and as models scaled up more the prefix-learning problem reared its head again. And so we get to the central motivation of the attention paper in the first place. From the paper: | A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus. | The authors propose a method to figure out what information matters for each word dynamically, thereby bypassing the information compression issue. | We start with an “attention” function. Generally, the attention function takes in two vectors and produces some number that represents a 'weight'. Often, that weight can be interpreted as a 'similarity'. In the paper, the attention function is a learned neural network that takes in some representation of each word (more on this in a second) and the current decoder hidden state and produces a vector of weights.1 The model runs a softmax over the attention vector, multiplies the word-representations by the corresponding attention-weights and adds them all together to get a final 'context' vector.2 | How do we actually get these word vectors? | One naive approach is to just run an RNN over the sequence and take the hidden state of each step as the corresponding representation for the word at that part of the sequence. But this runs into a variant of the same 'prefix' problem we mentioned earlier. The hidden state for the last word will have information about the whole sentence, while the hidden state for the first word will have none. More generally, the authors want the hidden state for each word to represent information on both sides of the sentence. | The trick is to simply run two RNNs. They run one RNN forward over the sentence and one backwards. For each step, they concatenate the two hidden states together to produce one 'word' vector. The forward running hidden state will have all of the information up till that point, x0 - i. And the backwards running hidden state will have all of the information after that point, xi - n.3 | One last note on architecture: beam search. | It may be weird to think about it this way, but language modelling is best thought of as a categorization task. Given some input, the model has to predict which of N categories is the most likely output. It's just that for language modelling, there are a lot of categories — one for each token, to be precise. As with all categorization tasks, the model does not output a single value. Rather, it outputs a probability distribution that represents the model's confidence of the output. | The authors are interested in text-translation, a subset of language modeling. They have some input text and they are trying to produce some output text. For each word, the model produces a probability distribution. If the output has 10 words, the model is actually producing a [10, N] matrix, where N is the number of tokens. We need to turn these into words somehow. | Naively, you could turn each probability distribution into a single word by simply looking at the largest value in that particular distribution. But this may lead to worse outcomes over the whole probability distribution (i.e. the whole sentence). For example, imagine you wanted to translate ‘quick brown fox’. The model may output ‘maroon’ instead of ‘brown’ as the most likely translation for the second word, which makes it much less likely to output ‘fox’ as the third word. | So the naive option has pitfalls. On the other extreme, you could try to maximize the probability of the entire sequence by looking at every combination of words. This is a NS operation, where N is the number of possible tokens and S is the sequence length. It's way too expensive. | So the authors use a technique called beam search. Instead of greedily choosing one word at a team, beam search aims to maximize the probability of phrases over the whole sentence. It's more accurate than simply running greedy, but less costly than doing a full search. | During experiments, the authors show a higher BLEU score on their translations, especially for longer sequences.4 They also show attention alignment results — they get some very cool graphs showing how their ‘attention model’ decides which words are important to which output words depend on which input words. | Even today, these sorts of graphs are the bread and butter of understanding what exactly language models are doing. | Insights | I tend to come at things from a representation learning perspective. The thing I'm generally thinking about when evaluating a deep learning architecture is ""what, exactly, is this model learning?"" It's not immediately obvious that Seq2Seq models learn prefix embeddings, but once that clicks all of the limitations of a traditional LSTM become immediately clear. | So from a representation learning perspective, the main innovation of this paper is the switch from learning sentence prefix embeddings to learning word embeddings and a reduce operation. Or, more generally, from learning sequence prefixes to learning set element representations. This is a powerful change. Prefixes are inherently unbounded, as a set increases in size so does the amount of information that a prefix embedding needs to store. That in turn means that a prefix embedding of a fixed size will lose resolution as the input gets larger. By contrast, word embeddings store a roughly constant amount of information. With attention, any set of weights within the model has to learn fewer things, and the overall complexity of what needs to be represented in any given embedding drops dramatically. | The attention mechanism is particularly elegant as a solution to the capacity problem, because it's not structurally limited to any particular kind of input information or problem space. In this paper the authors apply attention to learning word embeddings, but you could just as easily apply the general principles of attention to sentences, images, video…basically any kind of data. The reason attention is so flexible is because it is fundamentally a latent operation. It operates on embedding representations, which are essentially type-less. And as we've discussed in the past, attention in particular is also order invariant, which immediately makes it useful outside of sequential settings. | Other than that, the attention paper fits in with two larger patterns that we've seen across these papers. | First, information bottlenecks are bad. This is something we saw in both the ResNet paper and the LSTM blog post. If your model architecture requires all of your data to be funneled through a single stage, your model's representation capacity is effectively capped by the size of that stage. | Second, models seem to do better when you separate ""operations"" from ""representation"". This was something we discussed in depth in the identity mapping paper. It's not clear exactly when a particular set of weights is a representation and when it is an operation, but there's something there. | One last thought. | Attention at a micro level operates very similarly to the way people think about context and context windows at the macro level of LLMs today. One of the themes of the LLM Primer Series is that LLMs are pretty good at doing the right thing as long as they have context. Most of the challenge of LLMs is figuring out how to find and fit the right information into that context window. How do we find the right information? A lot of people turn to RAG systems that use some sort of similarity measure over a vector database to pull out and use key information. | But this is just attention! The RAG systems are implementing a larger form of attention over a bigger database, but that's all it is! Another way of framing this: attention is simply a vector search algorithm that is embedded within a model's weights. And as further evidence of this, the authors actually use search terminology. | Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. | The new architecture consists of a bidirectional RNN as an encoder (Sec. 3.2) and a decoder that emulates searching through a source sentence during decoding a translation (Sec. 3.1) | That search terminology carries all the way through to Transformers, and is where we get the Query/Key verbiage from. | Note that since it’s a translation task, you actually do have the whole sequence of the input a priori, i.e. you can ‘attend’ to ‘future’ words. | The phrasing in the paper is interesting — they treat it as a ‘separate model’. There was an era of ML where it was popular to talk about large ML models as if they were composed of smaller models. Often these papers would talk about “joint training”. I think we’ve dropped that language as an industry. A jointly trained model is just a large model. | I recently saw Tenet, which is a movie that makes zero sense, but I found it surprisingly useful as a metaphor to understand what these RNNs are doing. | BLEU is a quantification for how good a translation is. I think it’s mostly fallen out of favor as a quantification method, but don’t quote me on that — part of the reason it may no longer be used is AI has simply solved translation to a high enough level that we can’t really get more use out of it as a metric. | No posts | Ready for more?"
Building a keyboard for a 1985 Casio FX-451 calculator [video],1 point,zdw,youtube.com,https://www.youtube.com/watch?v=KUXgn2i8ETI,
Show HN: My New ASCII Editor,1 point,mikedesu,github.com/mikedesu,https://github.com/mikedesu/asciishade2,"We read every piece of feedback, and take your input very seriously. | To see all available qualifiers, see our documentation. | asciishade version 2 | Everything begins again... | asciishade version 2"
Show HN: Periodic Table and Wordle = Elemendle,1 point,lyl_003,elemendle.com,https://elemendle.com/,"Guess today's Periodic Table element - New element every 24h | Type any element name or symbol. The game will reveal its properties and show how close your guess was. | Period: | 1, 2, 3 | The period of an element in the periodic table is the horizontal row it belongs to | Group: | 1, 2, 3 | The group of an element in the periodic table is the vertical column it belongs to | Block: | s, p, d, f | The block of an element in the periodic table is the type of electron shell it belongs to | Type: | Metal, Nonmetal, Metalloid | The type of an element in the periodic table is the category it belongs to | Phase(STP): | Solid, Liquid, Gas | The phase of an element in the periodic table is the state of matter it belongs to at standard temperature and pressure | Atomic Mass: | 1, 2, 3 | The atomic mass of an element is the total mass of all its protons and neutrons | Atomic Number: | 1, 2, 3 | The atomic number of an element is the number of protons in its nucleus | Instead of guessing names, match element properties like period, group, block, type, phase, mass, and atomic number. | Access elements from all periodic table | Play at your own pace - take as many attempts as you need to find the right element. Elemendle is a game for the chemistry lovers, by the chemistry lovers. | Receive strategic hints after several attempts to help with challenging elements. | Monitor your statistics and export your data to continue playing on other devices. | Begin with well-known elements like Hydrogen or Oxygen to understand multiple properties at once. | Pay attention to the element's properties to narrow down possibilities. | Focus on phase and type hints as these often group related elements together. | Notice patterns in element properties to make more informed guesses. For example, if an element is a metal, it is likely to be a metal. | Don't hesitate to use the hints system after several attempts - it's designed to help! | No, Elemendle is completely free to play without any login requirements. Your progress is saved locally on your device. | A new element is available to guess every 24 hours. Come back daily for a fresh challenge! | Use the export feature to save your data, then import it on your new device. No account needed! | Green shows an exact match, red indicates no match, and arrows show if numeric values (like height) should be higher or lower. | Use the hints system after several attempts to help you guess the element. The hints are designed to help you, not hinder you. | Elemendle is a game for the chemistry lovers, by the chemistry lovers. It is a way to keep the periodic table community together and to celebrate the periodic table. | Yes, we are working on adding more play modes to Elemendle. | Currently, Elemendle works across on all devices with a browser. So, you don't need to download anything. | Your data is stored locally on your device. We do not collect any data from you. | Yes, Elemendle is completely free to play. | No, Elemendle is not affiliated with the periodic table. It is a fan-made game. | You can contact us at [email protected]."
Fill your GitHub calendar with fake commits to confuse recruiters,1 point,ofou,gist.github.com,https://gist.github.com/ofou/783034a293b68c5391da88def7cfb15d,"Instantly share code, notes, and snippets."
Building the Brain Simulator at Neuralink,1 point,houqp,poast.org,https://nitter.poast.org/neuralink/status/1880050812854104235#m,
"Metasyntactic variable beyond foo, bar, baz: quux, quux, corge, grault, garply",1 point,derwiki,wikipedia.org,https://en.wikipedia.org/wiki/Metasyntactic_variable,"A metasyntactic variable is a specific word or set of words identified as a placeholder in computer science and specifically computer programming. These words are commonly found in source code and are intended to be modified or substituted before real-world usage. For example, foo and bar are used in over 330 Internet Engineering Task Force Requests for Comments, the documents which define foundational internet technologies like HTTP (web), TCP/IP, and email protocols.[1][2] | By mathematical analogy, a metasyntactic variable is a word that is a variable for other words, just as in algebra letters are used as variables for numbers.[1] | Metasyntactic variables are used to name entities such as variables, functions, and commands whose exact identity is unimportant and serve only to demonstrate a concept, which is useful for teaching programming. | Since English is the foundation language or lingua franca of most computer programming languages, variables that originate in English are commonly seen even in programs and examples of programs written for other spoken-language audiences. | The variables used in a particular context may depend on subcultures that develop around programming languages. | Metasyntactic variables used commonly across all programming languages include foobar, foo, bar, baz, qux, quux, corge, grault, garply, waldo, fred, plugh, xyzzy, and thud.[1][3] Two of these words, plugh and xyzzy, are taken from the game Colossal Cave Adventure.[4] | A fuller reference can be found in The Hacker's Dictionary from MIT Press. | In Japanese, the words hoge (ほげ)[5] and fuga (ふが) are commonly used, with other common words and variants being piyo (ぴよ), hogera (ほげら), and hogehoge (ほげほげ).[6][circular reference] The origin of hoge as a metasyntactic variable is not known, but it is believed to date to the early 1980s.[6] | In France, the word toto is widely used, with variants tata, titi, tutu as related placeholders. One commonly-raised source for the use of toto is a reference to the stock character used to tell jokes with Tête à Toto.[citation needed] | In Turkey, the words hede and hödö (usually spelt hodo due to ASCII-only naming constraints of programming languages) are well-known metasyntactic variables that stem from popular humorous cartoon magazines of the 90's like LeMan. The words do not mean anything, and are used for precisely that reason. The terms were popularized more widely by the actor and stand-up comedian Cem Yılmaz in the late 90's and early 2000's.[7] | In Italian software programming culture, it is common to encounter names of Walt Disney characters (as found in the Italian versions of the shows) being used as variables. These names often appear in pseudo-code, are referenced in software engineering classes, and are commonly employed when explaining algorithms to colleagues. Among the most frequently used are ""pippo"" (Goofy), ""pluto,"" and ""paperino"" (Donald Duck). [8] | In the following example the function name foo and the variable name bar are both metasyntactic variables. Lines beginning with // are comments. | Function prototypes with examples of different argument passing mechanisms:[9] | Example showing the function overloading capabilities of the C++ language | Spam, ham, and eggs are the principal metasyntactic variables used in the Python programming language.[10] This is a reference to the famous comedy sketch, ""Spam"", by Monty Python, the eponym of the language.[11]
In the following example spam, ham, and eggs are metasyntactic variables and lines beginning with # are comments. | Both the IETF RFCs and computer programming languages are rendered in plain text, making it necessary to distinguish metasyntactic variables by a naming convention, since it would not be obvious from context. | Here is an example from the official IETF document explaining the e-mail protocols (from RFC 772 - cited in RFC 3092): | (The documentation for texinfo emphasizes the distinction between metavariables and mere variables used in a programming language being documented in some texinfo file as: ""Use the @var command to indicate metasyntactic variables. A metasyntactic variable is something that stands for another piece of text. For example, you should use a metasyntactic variable in the documentation of a function to describe the arguments that are passed to that function. Do not use @var for the names of particular variables in programming languages. These are specific names from a program, so @code is correct for them.""[12]) | Another point reflected in the above example is the convention that a metavariable is to be uniformly substituted with the same instance in all its appearances in a given schema. This is in contrast with nonterminal symbols in formal grammars where the nonterminals on the right of a production can be substituted by different instances.[13] | It is common to use the name ACME in example SQL databases and as a placeholder company-name for the purpose of teaching. The term 'ACME Database' is commonly used to mean a training or example-only set of database data used solely for training or testing. 
ACME is also commonly used in documentation which shows SQL usage examples, a common practice with in many educational texts as well as technical documentation from companies such as Microsoft and Oracle.[14][15][16]"
The best tool isn't always the most popular,2 points,thunderbong,rowsana.bearblog.dev,https://rowsana.bearblog.dev/the-best-tool/,"Home Blog | 17 Jan, 2025 | While big web apps like YouTube and Facebook existed, they were built by large companies. No one expected regular developers to create projects of that scale alone or even in small teams. | The idea that development is getting too complex comes from buying into the belief that we all have the same needs and resources as giant enterprises. | But in the real world, software takes time and there are so many control structures around the process of making the outcome wholesome and valuable. These include: methods, version control, tasks, estimates, designs, concrete experimentation, meetings, deadlines and man-hours. | Remember, a framework is meant to make your life easier and save you time. However, if your project is small, the time spent setting up the framework might take longer than the benefits it offers. Frameworks are great for larger web apps, making them more interactive. But for smaller projects, they can complicate things and lead to inefficient workflows. | Thanks to recent improvements, you can do a lot with just HTML and CSS, but it often seems like they aren't good enough for today's web needs. | Choosing technology for your stack is an exercise of picking your own poison. Either choose a paid service and be subject to vendor lock-in in the future, or choose an open-source one and pray that the community continues to maintain it. | Many people think that the main reason software projects feel dull is that developers spend a lot of time going through complicated processes to build features. However, I've noticed that even exciting projects can become boring. The real issue is how people experience working on these projects. | When we discuss system design today, we often neglect an important aspect: people. | We live in this tension between the ease of new platform features and the complexity of our stacks. | System design shouldn’t just focus on technical details like flowcharts. We spend time planning the software but forget to consider the people who build it. We try to fix problems with our tools but rarely think about the biases and limitations of the people involved. | There’s a lot of talk about processes—like sprint planning and team culture—and about the product itself—like requirements and customer needs. However, this doesn’t always lead to better software. Decisions often ignore the people making them, leading to poor outcomes. | “The divide is between people who self-identify as a (or have the job title of) front-end developer yet have divergent skill sets. On one side, an army of developers whose interests, responsibilities, and skillsets are heavily revolved around JavaScript. On the other, an army of developers whose interests, responsibilities, and skillsets are focused on other areas of the front end, like HTML, CSS, design, interaction, patterns, accessibility, and so on.” | — Chris Coyier | Prioritizing tasks is good, but it should help bring order to chaos. Simply estimating tasks isn’t enough; we also need to understand the risks involved. For software projects to succeed, we must consider not only processes and products but also the people involved. A balanced approach with all three—Processes, Products, and People—leads to better results. | Many problems have simple solutions at their core. But people are complex and you need people to solve problems.
Whenever you are faced with a problem, spend more time thinking through the people management side. | According to Sheena Iyengar, a business professor at Columbia University, decision making involves three distinct mental tasks: | Knowing What You Want: Gain clarity on your goals and priorities to direct your focus and evaluate options effectively. | Understanding What Options Are Available: Gather information about the range of choices, enabling informed decisions that align with your goals. | Making Tradeoffs Between the Available Options: Recognize that every choice involves compromise by weighing pros and cons to assess potential outcomes. | SWOT Analysis | Example: A software development company evaluates its services and products by identifying strengths such as a skilled development team and proprietary technology. Weaknesses might include high employee turnover. Opportunities could involve expanding into cloud services, while threats might include increasing competition from emerging startups. This analysis helps the company strategize its offerings and market positioning. | Decision Matrix | Example: When selecting a new project management tool, a development team creates a decision matrix to evaluate options based on criteria like cost, features, user experience, and integration capabilities. Each option is scored based on these criteria, allowing the team to compare projects quantitatively and showcase clear reasoning for their final decision. | Multivoting | Example: In a team meeting, developers brainstorm features for an upcoming software release. They list all suggestions, then use multivoting to narrow down the features by having each team member vote for their top three choices. This collaborative process helps focus the team on the most critical features based on collective input. | Pareto Analysis | Example: A software application is experiencing bugs. The development team collects data and discovers that 80% of reported issues stem from 20% of the code. By applying Pareto analysis, they decide to prioritize fixing those critical bugs first, leading to substantial improvement in application stability with minimal initial effort. | Multi-criteria decision analysis (MCDA) | Example: A company needs to choose between different technology stacks for a new application. Using MCDA, the team evaluates options—such as Node.js, Ruby on Rails, and Java—against various criteria like performance, scalability, community support, and development time. Each option is scored, allowing the team to make an informed decision that balances various needs and risks effectively. | It's really hard not to over-engineer web apps today because everyone wants to use what’s popular and worries about missing out. However, we should focus on our project goals and avoid letting our projects grow bigger than necessary. This applies to the tools we choose too; our decisions should be based on real needs, not just what others are using."
Ask HN: What's the easiest way to setup a scalable API developer platform?,2 points,ubutler,shotcut.in,item?id=42745904,
Check My AI Tool,1 point,Tanishmittal,utcc.utoronto.ca,https://shotcut.in/textbehindvideo,
Thoughts on having SSH allow password authentication from the Internet,1 point,zdw,andymasley.substack.com,https://utcc.utoronto.ca/~cks/space/blog/sysadmin/SSHOnExposingPasswordAuth,"On the Fediverse, I recently saw a poll about whether people left
SSH generally accessible on its normal port or if they moved it;
one of the replies was that the person left SSH on the normal port
but disallowed password based authentication and only allowed public
key authentication. This almost led to me posting a hot take, but
then I decided that things were a bit more nuanced than my first
reaction. | As everyone with an Internet-exposed SSH daemon knows, attackers
are constantly attempting password guesses against various accounts.
But if you're using a strong password, the odds of an attacker
guessing it are extremely low, since doing 'password cracking via
SSH' has an extremely low guesses per second number (enforced by
your SSH daemon). In this sense, not accepting passwords over the
Internet is at most a tiny practical increase in security (with
some potential downsides in unusual situations). | Not accepting passwords from the Internet protects you against three
other risks, two relatively obvious and one subtle one. First, it
stops an attacker that can steal and then crack your encrypted
passwords; this risk should be very low if you use strong passwords.
Second, you're not exposed if your SSH server turns out to have a
general vulnerability in password authentication that can be remotely
exploited before a successful authentication. This might not be an
authentication bypass; it might be some sort of corruption that
leads to memory leaks, code execution, or the like. In practice,
(OpenSSH) password authentication is a complex piece of code that
interacts with things like your system's random set of PAM modules. | The third risk is that some piece of software will create a generic
account with a predictable login name and known default password.
These seem to be not uncommon, based on the fact that attackers
probe incessantly for them, checking login names like 'ubuntu',
'debian', 'admin', 'testftp', 'mongodb', 'gitlab', and so on. Of
course software shouldn't do this, but if something does, not
allowing password authenticated SSH from the Internet will block
access to these bad accounts. You can mitigate this risk by only
accepting password authentication for specific, known accounts, for
example only your own account. | The potential downside of only accepting keypair authentication for
access to your account is that you might need to log in to your
account in a situation where you don't have your keypair available
(or can't use it). This is something that I probably care about
more than most people, because as a system administrator I want to
be able to log in to my desktop even in quite unusual situations.
As long as I can use password authentication, I can use anything
trustworthy that has a keyboard. Most people probably will only log
in to their desktops (or servers) from other machines that they own
and control, like laptops, tablets, or phones. | (You can opt to completely disallow password authentication from
all other machines, even local ones. This is an even stronger and
potentially more limiting restriction, since now you can't even log
in from another one of your machines unless that machine has a
suitable keypair set up. As a sysadmin, I'd never do that on my
work desktop, since I very much want to be able to log in to my
regular account from the console of one of our servers if I need
to.) | These are my WanderingThoughts 
(About the blog) | Full index of entries 
Recent comments | This is part of CSpace, and is written by ChrisSiebenmann. 
Mastodon: @cks 
Twitter @thatcks | * * * | Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web 
Also: (Sub)topics | This is a DWiki. 
GettingAround 
(Help)"
Using ChatGPT is not bad for the environment,72 points,returningfory2,reuters.com,https://andymasley.substack.com/p/individual-ai-use-is-not-bad-for,"If you don’t have time to read this post, these four graphs give most of the argument: | Why write this? | How should we think about the ethics of emissions? | Are LLMs useful? | Emissions | Water use | I’m not usually interested in writing simple debunking posts, but I regularly talk and read about the debate around emissions associated with AI and it’s completely clear to me that one side is getting it entirely wrong and spreading misleading ideas. These ideas have become so widespread that I run into them constantly, but I haven’t found a good summary explaining why they’re wrong, so I’m putting one together. | At the last few parties I’ve been to I’ve offhandedly mentioned that I use ChatGPT, and at each one someone I don’t know has said something like “Ew… you use ChatGPT? Don’t you know how terrible that is for the planet? And it just produces slop.” I’ve also seen a lot of popular Twitter posts (many above 100,000 likes) very confidently announcing that it is bad to use AI because it’s burning the planet. Common points made in these conversations and posts are: | Each ChatGPT search emits 10 times as much as a Google search. | A ChatGPT search uses 500 mL of water. | ChatGPT as a whole emits as much as 20,000 US households per day. It uses as much water as 200 Olympic swimming pools’ worth of water each day. | Training an AI model emits as much as 200 plane flights from New York to San Francisco. | The one incorrect claim in this list is the 500 mL of water point. It’s a misunderstanding of an original report which said that 500 mL of water are used for every 20-50 ChatGPT searches, not every search. Every other claim in this list is true, but also paints a drastically inaccurate picture of the emissions produced by ChatGPT and other large language models (LLMs) and how they compare to emissions from other activities. These are not minor errors—they fundamentally misunderstand energy use, and they risk distracting the climate movement. | One of the most important shifts in talking about climate has been the collective realization that individual actions like recycling pale in comparison to the urgent need to transition the energy sector to renewables. The current AI debate feels like we’ve forgotten that lesson. After years of progress in addressing systemic issues over personal lifestyle changes, it’s as if everyone suddenly started obsessing over whether the digital clocks in our bedrooms use too much energy and began condemning them as a major problem. | Separately, LLMs have been an unbelievable life improvement for me. I’ve found that most people who haven’t actually played around with them much don’t know how powerful they’ve become or how useful they can be in your everyday life. They’re the first piece of new technology in a long time that I’ve become insistent that absolutely everyone try. If you’re not using them because you’re concerned about the environmental impact, I think that you’ve been misled into missing out on one of the most useful (and scientifically interesting) new pieces of technology in my lifetime. If people in the climate movement stop using them they will lose a lot of potential value and ability to learn quickly. This would be a shame! | On a meta level, there’s a background assumption about how one is supposed to think about climate change that I’ve become exhausted by, and that the AI emissions conversation is awash in. The bad assumption is: | To think and behave well about the climate you need to identify a few bad individual actors/institutions and mostly hate them and not use their products. Do not worry about numbers or complex trade-offs or other aspects of your own lifestyle too much. Identify the bad guys and act accordingly. | Climate change is too complex, important, and interesting as a problem to operate using this rule. When people complain to me about AI emissions I usually interpret them as saying “I’m a good person who has done my part and identified a bad guy. If you don’t hate the bad guy too, you’re suspicious.” This is a mind-killing way of thinking. I’m using this post partly to demonstrate how I’d prefer to think about climate instead: we coldly look at the numbers, the institutions, and actors who we can actually collectively influence, and we respond based on where we will actually have the most positive effect on the future, not based on who we happen to be giving status to in the process. I’m not inclined to give status to AI companies. A lot of my job is making people worry more about AI in other areas. What I want is for people to actually react to the realities of climate change. If you’re worried at all about your own use of AI contributing to climate change, you have been tricked into constructing monsters in your head and you need to snap out of it. | Here are some assumptions that will guide the rest of this post: | If you’re not trying to reduce your emissions, you’re not worried about the climate impact of individual LLM use anyway. I’ll assume that you are interested in reducing your emissions and will write about whether LLMs are acceptable to use. | There’s a case to be made that people who care about climate change should spend much less time worrying about how to reduce their individual emissions and much more time thinking about how to bring about systematic change to make our energy systems better (the effects you as an individual can have on our energy system often completely dwarf the effects you can have via your individual consumption choices) but this is a topic for another post. | Our energy system is so reliant on fossil fuels that individuals cannot eliminate all their personal emissions. Immediately stopping all global CO2 emissions would cause billions of deaths. We need to phase out emissions gradually by transitioning to renewables and making trade-offs in energy use. If everyone concerned about climate change adopted a zero-emissions lifestyle today, many of them would die. The rest would lose access to most of modern society, leaving them powerless to influence energy systems. Climate deniers would take over society. Individual zero-emissions living isn’t feasible right now. | The average children’s hospital emits more CO2 per day than the average cruise ship. If we followed the rule “Cut the highest emitters first” we’d prioritize cutting hospitals over cruise ships—which is clearly a bad idea. Reducing emissions requires weighing the value of something against its emissions, not blindly cutting based on CO2 output alone. We should ask questions like “Can we achieve the same outcome with lower emissions?” or “Is this activity necessary?” But the rule “Find the highest emitting thing in a group of activities and cut it” doesn’t work. | In this post, I’ll compare LLM use to other activities and resources of similar usefulness. If you believe LLMs are entirely useless, then we should stop using them—but I’m convinced they are useful. Part of this post will explain why. | If climate change is an emergency that requires lots of people working collectively to fix in limited time, we cannot afford to get distracted by focusing too much of our effort and thinking on extremely small levels of emissions. The climate movement has seen a lot of progress and success in shifting its focus away from individual actions like turning off lights when leaving a room to big systematic changes like building smart grid infrastructure or funding renewable tech. Even if you are only focused on lifestyle changes, it is best to focus on the most impactful lifestyle changes for climate. It would be much better for climate activists to spend all their time focused on helping people switch to green heating than encouraging people to hang dry their clothes: | If the climate movement should not focus its efforts on getting individual people to hang dry their clothes, it should definitely not focus on convincing people not to use ChatGPT: | Another common concern about LLMs is their water use. This matters even though it’s not a direct cause of climate change. I’ll address that in the second part of the post. There might be other concerns as well (the supply chains involved in constructing data centers in the first place) but from what I can tell other environmental concerns also apply to basically all computers and phones, and I don’t see many people saying that we need to immediately stop using our computers and phones for the sake of the climate. If you think there are other bad environmental results of LLMs that I’m missing in this post, I’d be excited to hear about them in the comments! | Any statistics about the energy consumption of individual internet activities have large error bars, because the internet is so gigantic and the energy use is spread across so many devices. Any source I’ve used has arrived at these numbers by dividing one very large uncertain number by another. I’ve tried my best to report numbers as they exist in public data, but you should assume there are significant error bars in either direction. What matters is the proportions more than the very specific numbers. | If LLMs are not useful at all, any emissions no matter how minute are not worth the trade-off, so we should stop using them. This post depends on LLMs being at least a little useful, so I’m going to make the case here. | I think my best argument for why LLMs are useful is to just have you play around with Claude or ChatGPT and try asking it difficult factual questions you’ve been trying to get answers to. Experiment with the prompts you give it and see if asking very specific questions with requests about how you’d like the answer to be framed (bullet-points, textbook-like paragraph) gets you what you want. Try uploading a complicated text that you’re trying to understand and use the prompt “Can you summarize this and define any terms that would be unfamiliar to a novice in the field?” Try asking it for help with a complicated technical problem you’re dealing with at work. | If you’d like testimonials from other people you can read people’s accounts of how they use LLMs. Here’s a good one. Here’s another. This article is a great introduction to just how much current LLMs can do. | LLMs are not perfect. If they were, the world would be very strange. Human-level intelligence existing on computers would lead to some strange things happening. Google isn’t perfect either, and yet most people get a lot of value out of using it. Receiving bad or incorrect responses from an LLM is to be expected. The technology is attempting to recreate a high level conversation with an expert in any and every domain of human knowledge. We should expect it to occasionally fail. | I personally find LLMs much more useful as a tool for learning than most of what exists on the internet outside of high quality specific articles. Most content on the internet isn’t the Stanford Encyclopedia of Philosophy, or Wikipedia. If I want to understand a new topic, it’s often much more useful for me to read a ChatGPT summary than watch an hour of some of the best YouTube content about it. I can ask very specific clarifying questions about a topic that it would take a long time to dig around the internet to find. | What’s the right way to think about LLM emissions? Something suspicious a lot of claims about LLMs do is compare them to physical real-world objects and their emissions.  When talking about global use of ChatGPT, there are a lot of comparisons to cars, planes, and households. Another suspicious move is to compare them to regular online activities that don’t normally come up in conversations about the climate (when was the last time you heard a climate scientist bring up Google searches as a significant cause of CO2 emissions?) The reason this is suspicious is that most people are lacking three key intuitions: | The incredibly small scales involved in individual LLM use | How many people are actually using LLMs | Other online activities’ emissions | Without these intuitions, it is easy to make any statistic about AI seem like a ridiculous catastrophe. Let’s explore each one. | It is true that a ChatGPT question uses 10x as much energy as a Google search. How much energy is this? A good first question is to ask when the last time was that you heard a climate scientist bring up Google search as a significant source of emissions. If someone told you that they had done 1000 Google searches in a day, would your first thought be that the climate impact must be terrible? Probably not. | The average Google search uses 0.3 Watt-hours (Wh) of energy. The average ChatGPT question uses 3 Wh, so if you choose to use ChatGPT over Google, you are using an additional 2.7 Wh of energy. | How concerned should you be about spending 2.7 Wh? 2.7 Wh is enough to | Stream a video for 10 seconds | Watch an LED TV (no sound) for 3 minutes | Send 2 emails | Scroll on TikTok for about 4 minutes | Upload 30 photos to social media | Drive a sedan at a consistent speed for 15 feet | Leave your digital clock on for 3 hours | Run a space heater for 2.5 seconds | Print half a page of a physical book | In Washington DC where I live, the household cost of 2.7 Wh is $0.000432. | Sitting down to watch 1 hour of Netflix has the same impact on the climate as asking ChatGPT 300 questions in 1 hour. I suspect that if I announced at a party that I had asked ChatGPT 300 questions in 1 hour I might get accused of hating the Earth, but if I announced that I had watched an hour of Netflix or that I drove 0.8 miles in my sedan the reaction would be a little different. It would be strange if we were having a big national conversation about limiting YouTube watching or never buying books or avoiding uploading more than 30 photos to social media at once or limiting ourselves to 1 email per day for the sake of the climate. If this were happening, climate scientists would correctly say that the public is getting bogged down in minutia and not focusing on the big real ways we need to act on climate. Getting worried about whether you should use LLMs is as much of a distraction to the real issues involved with climate change as worrying about whether you should stop the YouTube video you’re watching 12 seconds early for the sake of the Earth. | Let’s take an extreme case and imagine that the reason you don’t want to use LLMs is that if everyone used LLMs over Google for every search, this would use too much energy. There are 8,500,000,000 Google searches per day. Let’s image that we replaced every single Google search with a ChatGPT search. That takes us from a daily energy use of 2,550,000,000 Watt-hours (Wh) to 25,500,000,000 Wh, or an additional 22,950,000,000 Wh, or 23 Giga-Watt-hours (GWh). The daily global energy demand from the internet is 2,200 GWh, so this would increase the daily global energy demand of the internet by 1%. A global switch from Google to ChatGPT would therefore be about the same as increasing the global population using the internet by 1%. If you heard that next year 1% more people would have access to the internet around the world, how concerned would that make you for the climate? Last year the actual growth rate of internet users was 3.4%. | In my experience using ChatGPT is much more useful than a Google search to the point that I’d rather use it than search Google ten times anyway. I can often find things I’m looking for much faster with a single ChatGPT search than multiple Google searches. Here’s a search I did asking it to summarize what we know about the current and future energy sources used for American data centers. It also saves me a lot of valuable time compared to searching Google ten times. | A lot of complaints about the total use of LLMs do not make sense when you consider the number of people using them. In considering LLM use, we can’t just look at their total emissions. We need to consider how many people are using the product. Someone could correctly point out that Google as a company produces way more emissions than a Hummer, but this is silly because Google has billions of users and the Hummer has one, and Google is very efficient with the energy consumed by each user. | Here are some examples to illustrate the point: | ChatGPT as of the time of writing has 300,000,000 daily users and 1,000,000,000 daily messages answered. Let’s imagine that you can snap your fingers and create one additional American household, with all its energy demands and environmental impact. This American household is special. The people in the household have one hobby: spending all their time writing very detailed responses to emails. They enjoy doing this and never stop, and they’re so good at it that they have 15,000 people emailing them every day, each person sending on average 3.3 emails for a total of 50,000 emails per day, or 1 email every 2 seconds 24 hours per day. People seem to find their replies useful, because the rate of use just keeps going up over time. Would you choose to snap your fingers and create this household, even though it will have the climate impacts of one additional normal American household? Seems like a clearly good trade-off. What if you had the option to do that a second time, so now 50,000 more messages could be answered by a second household every day? Again, this seems worth the emissions. If you keep snapping your fingers until you meet the demand for their message replies, you would have created 20,000 new American households and have 1 billion messages answered per day. 20,000 American households is about the size of the Massachusetts city of Barnstable: | If one additional version of Barnstable Massachusetts appeared in America, how much would that make you worry about the climate? This would be an increase in America’s population of 0.015%. What if you found out that everyone who lived in the new town spent every waking moment sending paragraphs of extremely useful specific text about any and all human knowledge to the world and kept getting demands for more? Of all the places and institutions in America to cut emissions, should we start by preventing that town from growing? | This number only really applies to our largest AI models, like GPT-4. GPT-4’s energy use in training was equivalent to about 200 plane flights from New York to San Fransisco. Was this worth it? | To understand this debate, it’s really helpful to understand what it means to actually train an AI model. Writing that up would take too much time and isn’t the focus of this post, so I asked ChatGPT to describe the training process in detail. Here’s its explanation.  What’s important to understand about training a model like GPT-4 is | It’s a one-time cost. Once you have the model trained, you can tweak it, but it’s good to go and be used. You don’t have to continuously train it after for anywhere near the same energy cost. | It’s incredibly technologically complex. Training GPT-4 required 2 × 10²⁵ floating point operations (simple calculations like multiplication, subtraction, multiplication, and division). This is 70 million times as many calculations as there are grains of sand on the Earth. OpenAI had to wire together 25,000 state-of-the-art GPUs specially designed for AI together to perform these calculations over a period of 100 days. We should expect that this process is somewhat energy intensive. | It gave us a model that can give extremely long, detailed, consistent responses to very specific questions about basically all human knowledge. This is not nothing. | It’s rarely this large and energy intensive. There are only a few AI models as large as GPT-4. A lot of the AI applications you see are using the results of training GPT-4 rather than training their own models. | It's helpful to think about whether getting rid of ""200 flights from New York to San Francisco"" would really move the needle on climate. There are about 630 flights between New York and San Francisco every week. If OpenAI didn't train GPT-4, that would be about the same as there being no flights between New York to San Francisco for about 2 days. That's not 2 days per week. It's 2 days total. Even if ChatGPT had to be retrained every year (and remember, it doesn't) that is less than 1% of the emissions from flights between these two specific American cities. How much of our collective effort is it worth to stop this? | 200 planes can carry about 35,000 people. About 20 times that amount of people fly from around the country to Coachella each year. There aren’t 20 AI models of equal size to GPT-4, so for the same carbon cost we could either cease all progress in advanced AI for a decade or choose not to run Coachella for 1 year so people don’t fly to it. This does not seem worth it. | To put a more specific number on the energy it took to train GPT-4, it’s about 60 GWh. GPT-4 was trained to answer questions, so to consider the energy cost we need to consider how many searches we have gotten out of that training energy use. I see the training cost as equivalent to comparing the cost of a shirt with how often you’ll wear it. If a shirt costs $40 and is well-made so that it will survive 60 washes, and another shirt is $20 but is poorly made so it will only survive 10 washes, then even though the first shirt is initially more expensive, it actually costs $0.67 per wear, while the second shirt costs $2 per wear, so in some meaningful way the first shirt is actually cheaper after you make the initial investment. In the same way, the training can look expensive in terms of energy if you don’t factor in just how many users and searches GPT-4 will handle. | A very rough estimate using publicly available data says that there have been about 200 billion ChatGPT searches so far. This means that so far, if we include the cost of training in the total energy cost of searching ChatGPT, we add 3 Wh/search to 60 GWh/200,000,000,000 searches = 3.3 Wh/search. The training cost distributed over each search adds 0.3 Watt-hours of energy, so it increases the total energy cost of a ChatGPT search by 1 Google search’s worth of energy. This does not seem significant. Consider now that ChatGPT is just one thing GPT-4 is being used for, other things include: | DuoLingo | Khan Academy | Be My Eyes | GitHub Copilot X | Once you factor in just how much use it’s getting, the energy cost of training GPT-4 looks incredibly cheap, in the same way that the more initially expensive shirt is overall cheaper than the second. | When someone throws a statistic at you with a large number about a very popular product, you should be careful about how well you actually understand the magnitudes involved. We’re not really built for thinking about large numbers like this, so the best we can do is compare them to similar situations to give us more context. The internet is ridiculously large, complex, and used by almost everyone, so we should expect that it uses a large portion of our total energy. Anything widely used on the internet is going to come with eye-popping numbers about its energy use. If we just look at those numbers in a vacuum it is easy to make anything look like a climate emergency. | ChatGPT uses as much energy as 20,000 households, but Netflix reported using 450 GWh of energy last year which is equivalent to 40,000 households. Netflix’s estimate only includes its data center use, which is only 5% of the total energy cost of streaming, so Netflix’s actual energy use is closer to 800,000 households. This is just one streaming site. In total, video streaming accounted for 1.5% of all global electricity use, or 375,000 GWh, or the yearly energy use of 33,000,000 households. ChatGPT uses the same energy as Barnstable Massachusetts, while video streaming uses the same energy per year as all of New England, New York State, New Jersey, and Pennsylvania combined. Video streaming is using 1600x as much energy as ChatGPT, but we don’t hear about it as much because it’s a much more normal part of everyday life. 20,000 households can sound like a crazy number when you compare it to your individual life, but it’s incredibly small by the standards of internet energy use. | Here’s how many American households worth of energy different online activities use globally, all back of the envelope calculations I did with available info, plus an equivalent American city using the same energy. I factored in both the energy used in data centers and the energy used on each individual device. There are large error bars but the rough proportions are correct. | 11,000 households - Barre, VT - Google Maps | 20,000 households - Barnstable MA - ChatGPT | 23,000 households - Bozeman, MT - Fortnite | 150,000 households - Cleveland, OH - Zoom | 200,000 households - Worcester, MA - Spotify | 800,000 households - Houston, TX - Netflix | 1,000,000 households - Chicago, IL - YouTube | Does this mean that we should stop using Spotify or video streaming? No. Remember the rule that we shouldn’t just default to cutting the biggest emitters without considering both the value of the product and how many people are using it. Each individual Spotify stream uses a tiny amount of energy. The reason it’s such a big part of our energy budget is that a lot of people use Spotify! What matters when considering what to reduce is the energy used compared to the amount of value produced, and other options to get the same service. The energy involved in streaming a Spotify song is much much less than the energy required to physically produce and distribute music CDs, cassettes, and records. Replacing energy-intensive physical processes with digital options is part of the reason the energy consumption per American citizen has gone down by 22% since its peak in 1979. | If people are going to listen to music, we should prefer that they do it via streaming rather than buying physical objects. Just saying that Spotify is using the same energy as all of New York City without considering the number of users, the benefits they’re getting from the service, or how energy efficient other options for listening to music would be is extremely misleading. Pointing out that ChatGPT uses the same energy as 20,000 households without adding any other details is just as misleading. | Here’s ChatGPT’s explanation of why and how AI data centers use water and where it goes after. In a nutshell, AI data centers: | Draw from local water supplies. | Use water to cool the GPUs doing the calculations (in the same way your laptop fan cools your laptop when it overheats). | Evaporate the water after, or drain it back into local supplies. | Something to note about LLM water use is that while much of the water is evaporated and leaves the specific water source, data centers create significantly less water pollution per gallon of water used compared to many other sectors, especially agriculture. The impact of AI data centers on local water sources is obviously important to think about and how sustainable they are mostly depends on how fragile the water source is. Good water management policies should help factor in which water sources are most threatened and how to protect them. | How to morally weigh different types of water use (data centers evaporating it vs. agriculture polluting it) seems very difficult. The ecosystems affected are too complex to try to put exact numbers on how bad one is compared to the other. I will say that intuitively a data center that extracts from a local water source but evaporates the water unpolluted back into the broader local water system seems bad for very specific local sources, but not really bad for our overall access to water, so the whole concern might be really overblown and wouldn’t matter at all if we just built data centers exclusively around stable water supplies. I’m open to being wrong here and would be excited to get more thoughts in the comments. Simply reporting “Data centers use X amount of water” without clarifying whether the water is evaporated, or returned to the local water supply polluted or unpolluted seems so vague that it’s a bad statistic without more context. | 20-50 searches on ChatGPT uses the same amount of water as a normal water bottle (0.5 L). | This means that it takes about 300 ChatGPT queries to hit 1 gallon of water. | The amount of water use by LLMs can seem like a lot. It is always shocking to realize that our internet activities actually do have significant real-world physical impacts. The issue with how AI water use is talked about is that conversations often don’t compare the water use of AI to other ways water gets used. | All online activity relies on data centers, and data centers use water for cooling, so everything that you do online uses water. The conversation about LLMs often presents the water they use as ridiculous without giving any context for how much water other online activity uses. It’s actually pretty easy to calculate a rough estimate for how much water different online activities use, because data centers typically use about 1.8 liters of water per kWh of energy consumed. This number includes both the water used by the data center itself and the water used in generating the electricity used. Here’s the water used in a bunch of different things you do on the internet in milliliters: | 10 mL - Sending an email | 10 mL - Posting a photo on social media | 20 mL - One online bank transaction | 30 mL - Asking ChatGPT a question | 40 mL - Downloading a phone app | 170 mL - E-commerce purchase (browsing and checkout) | 250 mL - 1 hour listening to streaming music | 260 mL - 1 hour using GPS navigation | 430 mL - 1 hour browsing social media | 860 mL - Uploading a 1GB file to cloud storage | 1720 mL - 1 hour Zoom call | 2580 mL - 10 minute 4K video | After the recent California wildfires I scrolled by several social media posts with over 1 million views each saying something like “People are STILL using ChatGPT as California BURNS.” They should have focused more on the people watching Fantastic Places in 4k 60FPS HDR Dolby Vision (4K Video). | Should the climate movement start demanding that everyone stop listening to Spotify? Would that be a good use of our time? | What about the water cost of training GPT-4? So far I’ve only included the cost of individual queries. A rough estimate based on available info says GPT-4 took about 250 million gallons of water, or about 1 billion liters. Taking from the assumption above that ChatGPT has received about 200 billion queries so far, the training water cost adds 0.005 L of water to the 0.030 L cost of the search, so if we include the training cost the water use per search goes up by about 16%. That’s still not as water intensive as downloading an app on your phone or 10 minutes of streaming music. Remember that ChatGPT is just one function that GPT-4 is used for, so the actual water cost of training per ChatGPT search is even lower. | Animal agriculture uses orders of magnitude more water than data centers. If I wanted to reduce my water use by 600 gallons, I could: | Skip sending 200,000 ChatGPT queries, or 50 queries every single day for a decade. | Skip listening to ~2 hours of streaming music every single day for a decade. | Skip 1 burger. | A common criticism of the above graph is that water cows consume exists in grass while water used in data centers is drawn from local sources that are often already water strained. This is not correct for three reasons: | Only 5% of beef cattle in the United States are grass-fed. | About 20% of US beef cows are in Texas, Colorado, and California. Each state has experienced significant strains on its water resources. Approximately 20% of U.S. data centers draw water from moderately to highly stressed watersheds, particularly in the western regions of the country, so on this it seems like both industries have roughly 20% of their total activity concentrated in places where they may be harming local watersheds. | Data centers pollute local water supplies significantly less than animal agriculture. | If you are trying to reduce your water consumption, eliminating personal use of ChatGPT is like thinking about where in your life you can cut your emissions most effectively and beginning by getting rid of your digital alarm clock. | A back of the envelope calculation tells me that the ratio of water use of 1 ChatGPT search compared to 1 burger is the same ratio as the energy use of a 1 mile drive in a sedan compared to the energy used by driving the world’s largest cruise ship for 60 miles. | If your friend were about to drive their personal largest ever in history cruise ship solo for 60 miles, but decided to walk 1 mile to the dock instead of driving because they were “concerned about the climate impact of driving” how seriously would you take them? The situation is the same with water use and LLMs. There are problems that so completely dwarf individual LLM water use that it does not make sense for the climate movement to focus on individual LLM use at all. | The people who are trying to tell you that your personal use of ChatGPT is bad for the environment are just fundamentally confused about where water (and energy) is being used. This is such a widespread misconception that you should politely but firmly let them know that they’re wrong. | Thanks for writing this, Andy! A point worth sharing here is that the environmental critique of LLMs seems to have been ""transferred"" from the same critique of blockchain technology and NFTs. An environmental critique of NFTs is, as far as I know, valid — selling 10 NFTs does about as much harm, measured in carbon emissions, as switching to a hybrid car does good. What may have happened is that two coalitions that were arguing with each other about one technology simply reused the same arguments when a newer, reputedly-energy-intensive technology came around. | This hypothesis is not my own, but it strikes me as extremely plausible. I couldn't see how otherwise critics of AI could have anchored on this argument, when there are many other perfectly valid arguments about the downsides of LLMs! | Nice post! | Could you give exact sources for the numbers in the ""Water consumed by ChatGPT vs other activities"" graph? I prefer to personally verify claims in infographics before resharing them, but sources like ""US Census Bureau"" and ""UNEP"" put out a _lot_ of data so it's not obvious which one of their reports I should look at. | No posts | Ready for more?"
Federal Reserve withdraws from global regulatory climate change group,2 points,zekrioca,github.com/nguyenchiemminhvu,https://www.reuters.com/world/us/federal-reserve-announces-exit-regulatory-climate-change-group-2025-01-17/,Please enable JS and disable any ad blocker
An introduction to Linux IPC techniques with practical examples,1 point,ncmv92,bloomberg.com,https://github.com/nguyenchiemminhvu/LinuxIPC,"We read every piece of feedback, and take your input very seriously. | To see all available qualifiers, see our documentation. | Examples of Linux Inter-Processing Communication | The Linux IPC feature provides the methods for multiple processes to exchange data and signals. | This guideline will introduce to you various IPC techniques, including File Locking, Pipe, Signal, Semaphore, Message Queue, Shared Memory, and Socket. | Each technique has its unique use cases and advantages. | File Locking is a mechanism to control access to a file by multiple processes, ensure that only one process can modify the file at a time, prevent data corruption. | This code below demonstrates a simple producer-consumer model using file locking for IPC in Linux: | Function to lock file | struct flock lock: Defines the lock. | fcntl(fd, F_SETLKW, &lock): Sets the lock, waiting if necessary. | Function to unlock file | lock.l_type = F_UNLCK: Specifies an unlock operation. | fcntl(fd, F_SETLK, &lock): Sets the unlock operation. | Full example source code HERE. | A pipe is an unidirectional communication channel that allows data to flow from one process to another. | It is commonly used for simple communication between parent and child processes. | Pipes are easy to use but limited to communication between related processes. | Find more demonstration HERE. | Unlike regular pipes, which are anonymous and only exist as long as the processes are running, FIFOs are given a name in the file system and can be accessed by unrelated processes. | Full example source code HERE. | Signals are a form of IPC used to notify a process that a specific event has occurred. They are used for handling asynchronous events like interrupts. Common signals include SIGINT (interrupt), SIGKILL (terminate), and SIGALRM (alarm). | I have prepared a sample code that demonstrates Signal technique. | Full example source code HERE. | The program manages a set of processes, monitor their heartbeats, and restart them if they become unresponsive. | Semaphores are synchronization tools used to control access to shared resources. They can be used to signal between processes and ensure that only a certain number of processes can access a resource at the same time. | Set of System V semaphore APIs on Unix-like operating systems: | semget(): Creates a new semaphore set or accesses an existing one. | semop(): Performs operations on semaphores, such as incrementing or decrementing their values. | semctl(): Performs various control operations on semaphores, such as setting values or removing the semaphore set. | Find demonstration source code HERE. | Message queues allow processes to exchange messages in a structured way. Each message is placed in a queue and can be read by another process. This method is useful for complex communication patterns and ensures that messages are delivered in order. | Set of System V message queue APIs on Unix-like operating systems: | msgget(): Returns an identifier for the queue. This identifier is used for subsequent operations. | msgsnd(): Send a message to the queue. | msgrcv(): Receive a message from the queue. | msgctl(): Perform various control operations on the message queue, such as querying its status or removing it. | Find demonstration source code HERE. | Shared memory is a method where multiple processes can access the same memory space. It is the fastest form of IPC because it avoids the overhead of copying data between processes. However, it requires careful synchronization to prevent data corruption. | Creation and Access: Shared memory segments are created using the shmget() system call, which returns an identifier for the segment. This identifier is used for subsequent operations. | Attaching to Memory: Processes attach to the shared memory segment using the shmat() system call, which returns a pointer to the shared memory. This pointer can be used to read from and write to the shared memory. | Detaching from Memory: When a process no longer needs access to the shared memory, it can detach using the shmdt() system call. | Control Operations: The shmctl() system call is used to perform various control operations on the shared memory segment, such as querying its status, changing its permissions, or removing it. | Find demonstration source code HERE. | Sockets provide a way for processes to communicate over a network. They can be used for communication between processes on the same machine or different machines. | Using in IPC context, Unix Sockets is powerful mechanism for communication between processes on the same machine. Unix Sockets don't need complex IP and port setup, they use file system paths to establish connections. | Full Unix-socket client-server example source code HERE. | https://github.com/nguyenchiemminhvu/LinuxIPC/tree/main | https://beej.us/guide/bgipc/html/ | https://opensource.com/article/19/4/interprocess-communication-linux-networking | https://opensource.com/article/19/4/interprocess-communication-linux-storage | https://opensource.com/article/19/4/interprocess-communication-linux-channels | https://www.linkedin.com/pulse/brief-linux-ipc-amit-nadiger/ | Examples of Linux Inter-Processing Communication"
Trump Team Has Wealth-Fund Ambitions for Small Lending Agency,2 points,petethomas,caseyhandmer.wordpress.com,https://www.bloomberg.com/news/articles/2025-01-17/trump-team-has-wealth-fund-ambitions-for-small-lending-agency,"To continue, please click the box below to let us know you're not a robot. | Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our Terms of
                Service and Cookie Policy. | For inquiries related to this message please contact
            our support team and provide the reference ID below."
The Los Angeles wildfires are self-inflicted,35 points,surprisetalk,caseyhandmer.wordpress.com,https://caseyhandmer.wordpress.com/2025/01/17/the-los-angeles-wildfires-are-self-inflicted/,"I don’t ordinarily write about events “in the moment” but for this I will make an exception, as I was personally affected. Caveats aside, my family and I are safe, we evacuated for several days, and due to heroic efforts by professional firefighters and psychotically brave neighbors, my house and most of my neighborhood escaped destruction. We were the lucky ones – by far. In 2019, as my wife and I were house hunting, we inspected multiple homes in the Pasadena area. Every house we looked at in Altadena burned to the ground last week. | I watched the Eaton fire burn across the face of the San Gabriel mountains from the window of a hotel we found, roaring upwind even against hurricane force winds until, by a twist of fate, around 1 am the winds died down and the fire fizzled out just outside the back doors of my neighbors across the road. | Let’s talk about fire – one of the four classical elements. | A fire requires three things to burn – fuel, heat, and oxygen. Various firefighting strategies attempt to address one side of the triangle. Water takes away heat, smothering takes away oxygen, and turning off the gas main or clearing brush takes away fuel. | Fire is a self-propagating reaction, in which fuels (typically wood, which is made of many copies of CH2O) react with oxygen to form CO2 and H2O, releasing heat. The rate of the reaction determines the size and speed of the fire. Chemical reaction rates are limited by available surface area, which is why twigs and kindling burn really quickly, while enormous tree trunks burn very slowly. A mountain that is covered in dry spindly grass and dead shrubs burns with great speed and heat, while a mature forest composed of tall living trees and no ground cover sometimes won’t burn at all. | The severity of a fire is often determined by where it occurs. A ground fire burns sticks and leaves that have fallen to the ground. A surface fire burns low-lying shrubs and smaller trees, separate from but still close to the ground. In extreme conditions and with the help of bridging fuel, a fire can jump from mostly dead leaf litter to the living crown of the forest – lots of tiny twigs surrounded by plenty of oxygen for combustion. | Remember, the living part of the tree, the cambium layer, is a thin surface between the bark and the core. In extreme heat, the water in the cambium boils, blowing the fire-protectant bark off the tree, creating even more fuel. | When I was a young child in Australia, a crown fire near my home on the New South Wales central coast was so severe running up one hill it not only burned the leaves (many Australian trees can survive this by sprouting more leaves called epicormic shoots) but also the trunk and root system, leaving only smoking holes in the ground. | Frequent ground fires can burn off fallen litter, keeping the forest floor relatively clear and preventing the level of fuel accumulation that can drive much more destructive fires into the crown of the forest. | The fire in Altadena appears to have become a “crown style fire” wherein the aerial fuel was not leaves and twigs but the frames and contents of houses ignited and broken open by severe wind gusts. | Los Angeles often gets strong winds from the northeast, called the “Santa Ana winds”. Although they vary in severity, every decade or so a major blow will knock down trees, cause power outages, and during fire season, accelerate fires. Los Angeles has a mediterranean climate, usually with wetter winters and dry summers. This year, however, the winter rains are yet to arrive, so the forest was unusually dry. | With a fire like the Eaton fire, at the peak of a wind storm and in a city already fighting the severe Palisades fire at the same time, once the fire is going the battle is already lost. The key is prevention! If the Eaton fire hadn’t started, then today thousands of residents in Altadena would be woodchipping a few fallen trees, fixing a damaged roof here or there, and getting on with their lives. | It’s hard to get your head around the scale of the destruction. Here are two recent videos from helicopter, a video of McDonald’s burning, and a video of Altadena on fire near Lake Avenue. | And here’s a still-incomplete map of the damage. | I worked at Caltech for five years and JPL for four years, I’ve lived in this area for 15 years. I’ve walked all these streets, hiked in the mountains hundreds of times. Dozens of people I know have lost everything. Beautiful streets, beautiful houses, incredible collections of art, music, literature. | I nearly bought this house, which is a mile and a half from the nearest forest. It had a pool in the backyard and was sheathed in stucco and non-combustible composite roof shingles. | Houses can be rebuilt, possessions repurchased or remade. As of January 16, 2025, 16 people were confirmed to have lost their lives in this fire, along with another 9 in the Palisades fire. More than 30 more are missing. | Let’s put the jigsaw puzzle together. This chart, from https://firemap.sdsc.edu/, shows how the Eaton fire fits between the scar of the Bobcat fire (2020 on the right) and the Station fire (2009 on the left). The northeastern edge of Altadena had not burned since 1993. | The unburned section in the middle near the Mt Lowe-Mt Wilson saddle is where the fire burned out and looks like this – not super combustible. | It is not a surprise that the Eaton fire was contained by the burn scars from previous severe fires, since one of the three sides of the fire triangle is fuel. Thanks to higher than historical levels of CO2 in the atmosphere, plants are growing faster than ever, especially if they have adequate rainfall. Even in relatively arid parts of the world like the Los Angeles mountains, photosynthesis leads to a predictable build up of fuel load in the form of fallen litter, shrubs, and trees. Every year, more fuel is added. After a year or two, the seed bed has sprouted and plants are growing. After five years, shrubs are dense enough that walking requires effort and evasion. After 20 years, fallen litter and growth can be so dense that lost hikers may require a helicopter to extract. | The biomass accumulation rate is something like 0.5 mm/year (1/64th of an inch), which doesn’t sound like much, until you realize that 20 years of accumulation is equivalent to coating the entire forest in a layer of gasoline ¼” thick. This is not hyperbole, drought resistant trees including eucalypts and creosote secrete volatile oils to help retain moisture, contributing to their combustibility. | This fact has not gone unnoticed by our ancestors. Taking just Australia and North America, authors including Percy Trezise, Dick Roughsey, Bruce Pascoe, and Charles Mann have extensively documented a variety of indigenous land management practices using fire. Early explorers in these previously-unknown-to-Europeans landscapes described forests that appeared like edenic gardens, because they generally were actually carefully curated and managed for both food production and security. | Burning off the undergrowth created space for grass to grow, attracting game for hunting. Fire was also used to “run” game off cliffs or onto rough or soft ground for easier and less hazardous hunting. Fire was also used as an offensive and defensive weapon against hostile neighboring tribes, underscoring the importance of keeping fuel loads low. Indigenous people didn’t have water bombing helicopters, so they burned the land on calm, cool winter days, in coordination with neighboring tribes, in small rotating patchworks to prevent catastrophic growth and spread. Bad fires could and did still occur, but people everywhere did whatever they could to prevent them and the fossil and historical record shows they were mostly successful. | Everywhere except in the contemporary West, where we imagine that our technological mastery excuses us from wild land curation and management. Even today, burning off the undergrowth every few years is a much more manageable approach to hazard reduction than, eg, the impractical army of laborers required to manually clear scrub on often extremely difficult terrain. | Let’s take some advice from founding father Benjamin Franklin. | “In the February 4, 1735 issue of the Pennsylvania Gazette, Benjamin Franklin wrote that “an ounce of prevention is worth a pound of cure.” Mr Franklin was not referring to medicine when he penned the now-famous line. He instead was referring to the importance of fire safety and the need for the city of Philadelphia to be better prepared to prevent and react to fires. In his article, he noted the importance of tending to how hot coals were being transferred in shovels (primary prevention), how chimneys should be cleaned regularly (primary prevention), and how a “club or society of active men” (firefighters) should be formed who can efficiently extinguish fires (tertiary prevention).” (https://www.medical.theclinics.com/article/S0025-7125(17)30056-1/fulltext) | Hazard reduction burns are not some crazy idea I just came up with. Indeed, even in California they are still conducted in a somewhat desultory manner. But given California’s century of experience with insanely destructive fires, we should ask why we don’t follow Franklin’s advice and spend more of the Golden State’s apparently inexhaustible wealth on prevention? | Before I get to the answer, I will dwell on an illustrative example from my native Australia, which has also suffered from unending catastrophic wildfires, following the Western tradition of modern agricultural and forestry practices. Australia also had problems with hazard reduction fires (“burn offs” or “back burns” in the local language) causing environmental problems, such as smoke, incidental damage to property, etc. Over time, rules accumulated about hazard reduction practices that increased friction to this essential activity and resulted in fewer burns. This reduced occasional problems with air quality, with the entirely predictable outcome that instead, unpredictable and intermittent catastrophes occurred, each with terrible environmental consequences that far outweighed the savings of ever-increasing regulatory pressure on hazard reduction activities.Afterall, it is possible to place an endless thicket of regulations on deliberate human activities, but one can’t simply outlaw wildfires and horrific destruction and loss of life. That requires planning and forethought. At the same time, safeguarding the environment for both human uses and wild animals requires reduction of severe fires. They’re not typically counted on statistics, but catastrophic fires that vaporize all the trees and sterilize the seed bed are bad news for the birds and kangaroos, too! | To take just one example, the Black Saturday fires of February 7, 2009 killed 173 people and destroyed thousands of structures, including substantially entire towns. | In an ongoing process of regulatory adjustment and reform, the New South Wales fire service most recently adopted rule 10/50, which gives homeowners the right to clear trees within 10 meters of a home and non-tree vegetation within 50 meters of a home, without approvals. No friction! | In contrast, in the US, well-intended but poorly considered rule changes frequently make it harder to comply with regulations and increase friction, reducing the ability to do the obviously correct thing. | For federal- and state-funded fire prevention efforts, laws like NEPA and CEQA are often weaponized by special interest advocacy groups, resulting in ludicrous outcomes such as the US Forest Service spending 40% of its budget (!) on permitting-related activities. | NEPA, a law passed in 1970, was intended to help protect the environment. Subsequent regulatory interpretation (now in doubt due to Chevron being overturned) has expanded the law from just five pages of text to an overwhelming blizzard of rules that have brought our ability to build things to a grinding halt. California’s local version of NEPA, CEQA, similarly ties the hands of state agencies whose very mission is preventing all kinds of natural disasters from destroying our cities. | Here’s a summary of the delays wrought by “environmental protection” legislation and process on wildfire hazard reduction. | This is nationwide, not just unique to California. Remember, after 5 years even dry chaparral scrub has accumulated enough biomass to burn well – and it literally takes longer than that to complete an Environmental Impact Statement review. Here’s a more comprehensive list of forests that literally burned down while their forest-protection hazard reduction burns were held up by NEPA reviews, often for 5-7 years! If you’re wondering what the Platonic Ideal of a failed law supported by failed regulatory policy and a failed state looks like – this is it! | Explain this to the tens of thousands of law abiding tax paying citizens in Los Angeles who lost their homes this week. Explain to them why this process, aided and abetted by layer after layer of corrupt, self-serving regulators/consultants/affiliates/NGOs/501c3 charitable organizations, is fair or reasonable. Maybe we can take some of the millions of printed pages of these nonsense, useless “Environmental Impact Statements” and use them to build a new roof over their heads. | This is not a new realization. Governor Newsom, in office now for six years, attempted to formulate a streamlined CEQA workflow for fire hazard reduction burns, but was forced to concede that complying with the new workflow is even slower and more onerous than the default option. Under his leadership, California expanded its fleet of fire fighting tanker aircraft, but in the strong Santa Ana winds last week, they were unable to fly due to severe turbulence. Once the catastrophic fire has started, it is too late. As Ben Franklin said, prevention is better than cure. | Even if the tankers can fly, the bright pink fire retardant Phos-Chek is not super great for the environment. | Why does the process chart look like this? It’s not really a process chart – it’s an org chart. Why does handling a US Forest Service proposal to conduct a hazard reduction burn take five years? Every arrow and box on this org chart represents an unaccountable, unelected, unfireable fiefdom. It is the epitome of rent-seeking “job security” for a class of worse-than-useless bureaucrats whose only job is to keep our cities safe from fire, and whose collective self-serving glacially slow BS self-evidently continues to be prioritized over keeping citizens safe. | What happens when well-meaning regulatory reform, such as that described above, occurs? Various factions of the US governing layer attempt to settle their differences in court, often at extreme cost. This is complicated by the fact that huge swaths of land in the US west is federally owned and managed, with every kind of overlapping jurisdiction perversity you can possibly imagine. | Here’s Isaiah Taylor’s summary of a few legal actions taken against common sense hazard reduction burns. | Sierra Club (better known for accepting bribes large undisclosed donations from the gas industry) successfully litigated the US Forest Service over their attempt to create a NEPA categorical exclusion. The Center for Biological Diversity informed the BLM they would sue over a fuel reduction plan. | I do not know exactly why these ostensibly environmentalist organizations move in lockstep to block these and other actions that actually protect the environment, including the development of lower carbon power sources. It could be ignorance or it could be some fundamentalist underlying anti-humanist ideology – who am I to say? It is not unusual to hear shards of the environmentalist movement make claims like “those houses should never have been built there” or “humans are a blight on the face of the universe” as though that excuses positive actions taken to prevent high efficacy hazard and harm reduction measures. | Various regulatory reform efforts failed to pass both houses at both state and federal levels. | While we’re listing various regulatory issues in need of reform, in addition to NEPA/CEQA and the horde of parasitic opportunistic so-called advocacy groups that weaponize the laudable democratic features of these laws against the tax paying citizens they are intended to protect, we also have (not exhaustive list – let me know which ones I’ve missed!) | The California Coastal Commission, an unelected junta of “nyet men” who reflexively block development of any coastal infrastructure, including the sort of large-scale desalination that could ease California’s water shortages and provide enough water to a) irrigate land near human population centers thereby reducing combustibility and b) fight fires when they occur. | The Federal Aviation Administration, whose ongoing intransigence on commercial drones has handed China a 20-year lead on this critical technology and also denied the US their application in forest management and fire fighting. | LA county zoning, which continues to exacerbate a critical housing shortage by preventing infill development in less hazardous areas and forcing people to live in areas prone to natural disasters. This is another example of “death by a thousand cuts” where a continually growing thicket of regulations that help one constituency (landed homeowners) by artificially inflating the value of their assets at the cost of another constituency (typically younger renters whose family formation rates are severely hampered by a lack of affordable housing) leads to much more expensive consequences in the longer term (demographic crisis, non-conforming ancient housing stock, and disaster-prone communities). These trade offs require leadership who can sell the necessity of decisions that prioritize the needs of the many over the needs of the few or the one – leadership that is in high demand and short supply! | Fire resource management, which apparently left crucial reservoirs empty for uncompleted maintenance for well over a year. Much of this infrastructure was built a century ago in less time and for less money than it supposedly costs the taxpayer today to keep it in barely working order. There’s a huge grift going on here. | All this discussion may come as a surprise to anyone who has taken a US civics class, and erroneously believes we live in a representative republican democracy, wherein elected representatives make laws that govern the free commerce between people, businesses, and their government. Afterall, at no point in the federal or California state code does it say “under no circumstances are hazard reduction burns allowed, as it is the official government policy that forests be allowed to accumulate fuel for decades until massively destructive fires are unavoidable”. And yet, here we are. So how did this happen?If we follow Charlie Munger’s advice to “invert”, let us instead understand that the law is the necessarily poorly defined set of rules and regulations that govern how the state is actually run, and bears, at best, a passing resemblance to the millions of pages of written federal and state code and accompanying regulations. In this analysis, it is quite clearly the law that hazard reduction burns are illegal, since despite their utility being extremely high and their cost almost trivially low (recall that indigenous societies that lack writing and money seem to be able to allocate the necessary resources for their successful execution) they do not occur at anything like the rate necessary to provide actual protection to people and property. | Who, then, governs us? Who is actually making law? | The answer is a handful of unelected specialists in bureaucracy. The key decision makers within the US Forest Service, CALFIRE, the BLM, the CCC, and their symbiotes at the Sierra Club, The John Muir Project, the Center for Biological Diversity and so on could fit quite easily onto a single Greyhound bus. You don’t know their names. They never appear on TV. They are unelected. Their position is often based on seniority rather than merit. They are effectively unfireable and thus unaccountable to anyone, not even the duly elected Governor. Generally speaking their salaries are far from exceptional, particularly in the public service and non-profit space. They’re not doing it for yacht money and it’s a fair assumption that for most of them (as well as most of us), a career in the upper echelons of business or private industry would be unattainable. Generally they are well-intentioned first order thinkers whose local ideological gradient and surrounding incentive landscape dictates their actions with eerie predictability. All of them are opinionated as to the cause and responsibility for these fires, and none of them will point a finger inward. These are the people who make the law that governs you. These are the people that claim de facto authority to control nearly every aspect of your life. These are the people whose individual and collective incompetence resulted in your house burning to the ground. | As of January 16, the dust hasn’t quite settled but between the Eaton and Palisades fires, around 10000 structures have been destroyed and 25 people killed, with about 38,000 acres burned. The cost is expected to exceed $50b, partly because Los Angeles’ legacy of short-sighted zoning decisions has driven up housing costs, and thus the costs of all services provided by people who need to live in houses, including construction, policing, and firefighting. It’s almost like housing is a 10,000 year old technology that we’ve made artificially scarce for no good reason. | Let’s discuss the current response by Mayor Karen Bass and Governor Gavin Newsom, starting with early weather predictions. | For years fire fighting authorities, aware of the central dereliction of responsibility in hazard reduction, have warned about a lack of funding, staffing, and resources. Instead of solving these problems, instead we received saturation media coverage for days in advance warning of strong Santa Ana winds and high fire risk, in the form of “red flag” warnings. | What was the city’s response to red flag warnings? Mayor Bass took off on a junket to Africa and civilians raked their yards and furtively watered their lawns against water conservation regulations that are expected to make up for a half century of under-investment in water procurement infrastructure. At this point, it was too late – the die was cast. The time to conduct hazard reduction burns safely on LA’s hills was years, or decades before. Or even during the previous and much wetter winter. By the time the wind was picking up, the fates of Pacific Palisades and Altadena were sealed. | Who else paid attention to saturation media coverage of red flag warnings? Evidently, a city full of deranged arsonists began to hyperventilate and frantically collate their kits of accelerants and blow torches. ABC7 reported at least 300 fires were started intentionally during this period, stretching already underfunded, understaffed firefighting resources even further. Some crazy people love to light fires, evidently, and many more began systematic looting operations in evacuated neighborhoods. | There’s obviously a broader conversation to be had here about mental illness and criminality, but media generally doesn’t publicize suicides because doing so is known to induce copycat behavior. It’s not exactly a surprise to find that while 99.9% of people respond to “fire danger” warnings appropriately, some fraction reach for the matches. Perhaps, instead of spending public resources multiplying colors and flags of public safety warnings, we could ask the state that already taxes us so heavily to just solve the public safety issues. Afterall, they have a monopoly on legitimate violence. | During an official press conference on January 9, someone somehow managed to send an emergency evacuation order to all 10 million residents in LA County – a new high water mark for gross misuse of this tool. Instead of being woken in the middle of the night by a phone buzzing about a custody dispute 100 miles away, we had millions of frightened people further terrorized by a governing apparatus that not only can’t manage basic fire prevention, they also can’t build a public notification system without pushing the big red button by accident! [Three times!] | Haha, whoopsie! Who amongst us hasn’t accidentally panicked the second largest city in the US? One begins to wonder if this sort of basic system level dysfunction isn’t the biggest frustration experienced by Chinese hackers attempting to implant doomsday backdoors in all our critical infrastructure. | Okay, what else can city, state, and federal officials do about this disaster? We got a whole series of desperate and poorly conceived reactive policies, none of which address the root causes of the fire, are able to do much about the losses of the fire, or will do anything to prevent the next fire. | Price controls on insurance | Since 1988 under Prop 103, the Californian insurance industry has been heavily regulated. In practice, this means that insurers can calculate risk premiums but, like regulated utilities, need government permission to raise rates. Raising rates is politically unpopular, so insurers tend to cancel policies they can no longer justify, many of them in the areas that just burned. Insurers are in a bind, because their risk is driven up by government inaction on risk reduction, and their ability to recover costs is crushed by that same government. They leave the market. The CA state government’s response is to form a publicly funded insurer, which recently was barely solvent with about $200m in reserves, or about $20,000 per lost structure. This is not going to cover losses, to put it mildly. | Some out-of-state insurers operate in California, and charge an excess of over $100,000 on home insurance – an accurate estimate of the true risk being run by homeowners. Relatives in Australia are reporting that insurers there are already affected by contagion from the LA fires and are jacking up renewal rates. The CA state response to insurers exiting this dysfunctional market in droves is not to allow them to charge a fair premium for the risk they’re forced to hold, but to ban insurers from dropping coverage for anyone in the affected zones for another year. On the one hand – they’re not going to burn again. On the other, this can only accelerate the major insurers – if they survive – pulling out of the state as soon as they can. | I don’t think it’s reasonable to expect CA taxpayers from across the state to end up holding the bag here, but I think this is also a cautionary tale about 1) the threat of demagoguery via direct democracy and 2) excessive market manipulation by regulators. Price controls create scarcity. In an ideal world, CA regulators would interface with the insurance industry by ensuring that it stays competitive, by lowering barriers to entry, and most importantly by listening to their highly trained actuaries about which policy changes would reduce risk. Note that this is in marked contrast to how insurance has actually been run for the last 36 years, with predictable results. | Much more comprehensive thread from Kim-Mai Cutler, and another from Patrick “patio11” “dangerous professional” McKenzie, and another from Ian Gutterman. | Price controls on rentals and construction | Mayor Bass zoomed back from her trip to Ghana to attend a series of agonizingly embarrassing press conferences, her city anxious to hear how she would lead them beyond this crisis. Her response: price controls on rentals and construction. At a time where LA’s already catastrophically stressed supply of housing has shrunk even further, with more than 100,000 people displaced by home losses and evacuations, we desperately need additional supply brought into the market. The genius of a market capitalist system is that supply is elastic and allowing prices to increase (e.g. Uber surge pricing) brings additional supply into the market. Provided barriers to entry are low, additional supply is also competitive, limiting price increases. Prices are information that solve the Hayek knowledge problem. | The fires are a horror. It is a different kind of horror to come slowly to the realization that the mayor of the second largest city in the US is somehow unaware of the basics of supply and demand, or that fixing prices creates artificial scarcity. With just this one economically illiterate policy, Mayor Bass has greatly exacerbated the housing shortage. Houses don’t come from nowhere! Construction crews don’t spring from the Earth fully formed!We’re going to need a lot of construction capacity to rebuild these parts of LA. It would be great if some companies that specialize in commercial construction switched over to residential, or if some companies that operate in other cities came here to increase capacity. It seems unlikely that they could change their operations like this at zero cost – so that cost must be borne by the consumer. But if Mayor Bass believes, like the Soviet Politburo, that prices should be set by fiat, or whatever they were last Thursday, and enforces her belief with policy, she has simply zeroed out any incentive for these additional construction crews to come to LA. Why would they, knowing that if they offer their honest and desperately needed services at a reasonable price, negotiated between free people, Mayor Bass is prepared to single them out for blame on this disaster that, unlike her, they bear no responsibility for. Instead, she should be finding ways to reduce friction on licensing and construction approvals! | Mayor Bass, point the finger inwards and publicly revoke price control measures, if you care about your city! | Price controls on land sales | Not to be outdone, Governor Newsom dropped an EO on “predatory real estate sales”. This is just weird. In a situation where tens of thousands of people are still evacuated, and thousands more are now homeless, fires are still burning, and the incandescently stupid regulations that led directly to dozens of destructive fires including these ones still stand, apparently the problem worthy of a press conference is people offering to buy ruined houses – in a state where rebuilding will probably take years. Any landowner can always say “no”. | Speculation has swirled as to whether this is an attempt to make wrecked neighborhoods harder to depopulate, since both Pacific Palisades and Altadena are relatively high income communities full of the sorts of taxpayers who have been fleeing California in recent years. | Or perhaps the land changing hands without approved new construction would lock in lower Prop 13 land tax revenue than the state was counting on. | But we should probably not ascribe to malice that which can be explained by incompetence. | Governor Newsom harbors the least secret presidential ambitions in the history of the world, so let’s see what else his office has to offer during this trying time for Angelenos? | A stern warning about looting | Prior to activating the national guard, Governor Newsom warned prospective looters that theft was illegal – as if they didn’t know that numerous CA DAs were no longer prosecuting “petty theft” under $950 – a mind-boggling policy that has all but destroyed retail in downtown San Francisco. Incredibly, city officials there have responded to business owners going bankrupt from incessant theft, not with spending their billions of dollars on law enforcement and public safety, but with yet another blizzard of regulations banning owners of failed businesses from closing up shop without permission from the same city government that has snoozed through years of high taxes and zero protection of private property. It’s exactly the same playbook now being used to punish insurers! | I also found it particularly droll to get a lecture on looting from the governor of a state which takes up to 14.4% income tax from everyone but can’t even amend CEQA to prevent its most prosperous cities from burning to the ground. | Suspension of CEQA, but only for burned areas | Governor Newsom signed another EO, this time suspending CEQA and the Coastal Act, and also throwing in some price controls for construction costs, just to make sure that contractors are properly punished for trying to build, even outside of Mayor Bass’ jurisdiction. | This EO also broke my brain. Apparently the state will fold like a house of cards over and over and over again in the face of petty bullying from the Sierra Club when it comes to selective enforcement of CEQA and the Coastal Act on routine hazard reduction burns, green energy development, desalination, infill development, rezoning, permitting reform, and any number of other existential crises that plague this state. | But, when it comes to rebuilding a few residential neighborhoods whose very fate was sealed by these laws, the Governor can just cancel them if he feels like it? There is a caveat though, the rebuilding must be substantially similar to the pre-existing structures. No densification, no planning-level natural disaster hazard reduction, no infill development is allowed! Housing must remain catastrophically scarce across the state! | Fancy claiming the authority to defang CEQA or the CCC by EO and wasting it on something so trivial. Governor Newsom, you could have blanket approved upzoning, or construction of 5 TW of solar, or desalinating enough water to recover Central Valley agriculture, the Owens Valley, and develop the Imperial Valley. | A check from Joe Biden for $770 | At this point I don’t think anyone knows who’s actually running the Biden administration, but the oddly specific number of $770 has been generated for people affected by the fires. This is a nice gesture that will cover about two nights in a hotel and a change of clothes, but can I not cash the check in return for NEPA being deleted? I would frankly rather my friends still had their houses. It’s not like NEPA is critical infrastructure for some other pillar of society. How much pain do we need to learn this lesson? | How do any of these policies and interventions make it easier to build? How do they reduce friction to “do the right thing”? At this point, I don’t know a single person who can accurately estimate the final costs of this disaster, or how long it will take to rebuild the affected areas. Given that the insurers are, by default, now globally insolvent, and that the Trump administration is unlikely to bail them out for 100 cents on the dollar, and that local and state leadership seems to be sending the strongest possible signal (through attempted price controls) that expedient rebuilding will be blocked at every turn, it is possible that these areas will never be rebuilt. | As for Los Angeles, we can easily predict future fires in areas that haven’t burned. Let’s put on our insurance actuary hat and look at the map. | These maps are adapted from https://firemap.sdsc.edu/, which has a layer for historical fires. | The areas above in red have burned historically and will burn again, but haven’t burned in the last 5 years. Typically after five years, there is enough regrowth to sustain a burn in mild conditions. These are the areas that should be divided into discrete patches and then systematically burned off every 5-10 years on a rotating schedule, before their fuel loads grow beyond the point where safe hazard reduction is possible with fire. Once this patchwork is established, growth of megafires even in extreme conditions is practically impossible, since any fire will be largely surrounded by more recently depleted zones. | The areas above in red have burned historically and will burn again, but haven’t burned in the last 20 years. Typically after 20 years, there is enough regrowth that it can be difficult to sustain a safe burn, even in mild conditions. These areas are ticking time bombs. You and I and everyone else can see on this map, or on a hike through these areas, that government indifference and incompetence has left anyone who lives near these areas vulnerable to a disaster that will definitely occur, and most likely within the next decade. Given sufficiently mild weather, it may be possible to burn these areas safely in the hours before a predicted rain storm, but timely assurance of safety may only be possible through the enormously expensive manual removal of brush through tens of thousands of acres of incredibly rough terrain – an action which is also blocked by default by CEQA, unfunded by default, and if history is any guide, not going to happen. Your insurers are looking at this same map, so if State Farm dropped you – that’s why. I don’t think FAIR will be able to pick up the tab anymore either. | How are we supposed to go forward and scrape out an existence here on the edge of the Pacific Ocean? Our forebears understood that California was a half-finished land, a wild land of natural beauty and possibility, but one that required human vision and care to complete. The modern California exists only because of the labors and foresight of people who lived long before we were born, in whose manifested paradise we grew, and whose man-made underpinnings we have failed to understand, value, or maintain. Fires and droughts are natural, but we bear responsibility for their effects on us. If the skies don’t give us enough rain, we can easily make more water – if we choose. It is cheap compared to our present level of wealth and technical accomplishment. If the mountains give us fire, we can curate and control their fuel levels so our children can sleep in peace knowing that their neighborhood is safe from man-made disasters. | How many more cities must burn before we decide we have suffered enough? | If only it was that simple. | First, prescribed burns are difficult, dangerous, and sometimes nearly impossible to do safely – particularly in areas where numerous houses are in or near woodlands. Some of the most destructive recent fires were prescribed burns that didn’t go as planned. Nobody knows what the optimal frequency is: there is growing evidence that doing them just a little bit too often has caused catastrophic environmental degradation over huge swaths of Australia’s Northern Territory. | Second, prescribed burns wouldn’t help in this particular case. Climate  change is making California’s climate increasingly bimodal. There were two very wet years, followed by rapid drying of accumulated vegetation. By the time the abnormal and out-of-season Santa Ana winds showed up in the forecast, it was too late to organize burns even if there were no permissions required at all. | Third, in the particular case of Californian chaparral, prescribed burns don’t help in general. It’s a plant community that naturally burns once in a while, and always will. People just shouldn’t live there, unless thay are fine with living in temporal structures like the Native Americans did. | Finally, if you want someone other than the fossil fuel industry to blame, blame those Native Americans. They killed off the megafauna that once kept the chaparral more open and fragmented. The rest was inevitable. | LikeLike | Oh, and one more thing. California is not the only place with seasonally dry subtropical climate. There are many others, some with more bureaucracy, some with less. But every one of those places has recently experienced disastrous wildfires with huge loss of life and property. Southern Europe, Turkey, the Maghreb, the Levant, central Chile, southeastern Australia, southern South Africa – all of them. Russia has only a tiny patch of Mediterranean vegetation on the Black Sea coast, and even that burned completely a few years ago. And it will keep getting worse. | LikeLike | I’m not sure if you are correct about the controlled burns – I have heard several views on that | However I am sure that you are wrong about the root cause of the problem | The root cause is a failure to distinguish between the “leaders” and the “doers” | Back in the beginning the USA started with a great idea | The “Preamble to the Constitution” | This was the most important part of the Constitution – it was a “Purpose Statement” | The Preamble laid out in simple terms what the Constitution was FOR – its “Purpose” – the Lense that the constitution and all subsequent legislation should be observed through | Unfortunately they lost the plot very soon afterwards and people started ignoring the Preamble | Improvement (1) – All Legislation should start with a “Purpose Statement” | Improvement (2) – Distinguishing between the “Leaders” and “Doers” | Leaders make the important decisions – set the directions | Once that is done then the “Doers” turn those into actual actions | Legislation – from Congress or the State equivalent – should be short with details about the expected results and less detail about the exact mechanisms | 20 pages MAX | The hundreds of pages of actual regulations required should be written by people closer to “the action” | This has the big advantage that if (when) it needs to be changed and improved this can be done without going back to Congress | The law courts and Judges should use the “Purpose Statement” when people challenge any part of the regulations | One of the American “failings” is a strong “Not Invented Here” sentiment | IMHO this is silly – if somebody else has a good idea you should grab it and file the ID marks off it | With that in mind this is how its done here (NZ) | Laws are written by Parliament | The process | The need for some legislation is found | A committee is appointed to look into it – they do an initial look and publish their findings on the Net – an important part of that is a “Purpose Statement” | We can all comment on that – in person or on-line | The comments and submissions are all gathered together and the committee goes back to work – and produces a final document – which goes on-line for more comments | We get a few months for comments – then the comments and the document are submitted to Parliament (which can amend) and if accepted become law | This is very different from the US system – each Act is a separate “solution” to a problem – there is no hanging unrelated items onto it | And the “Purpose Statement” – is critically important – it tells the courts what was INTENDED – which is useful when the inevitable faults appear | This is the TOP LEVEL law – regulations are below that level and are written by the Bureaucrats – but that means that they can easily be updated | That is the USUAL process – there are “fast track” methods of flying legislation through Parliament – but they are not as good and only used when speed is required | LikeLiked by 1 person | I largely agreed with 95% of this and I was excited to potentially share this piece with others, I enjoy a good NEPA bashing, but about half-way through I was put-off by a less-explained maliciousness in the quoted section below towards bureaucrats that comes off like a stereotypical “hate the guberment” segment. Maybe more clearly outline how the specific bureaucrats are at specific fault here to explain what warrants disdain and/or lighten it up. | ”All of them are opinionated as to the cause and responsibility for these fires, and none of them will point a finger inward. These are the people who make the law that governs you. These are the people that claim de facto authority to control nearly every aspect of your life. These are the people whose individual and collective incompetence resulted in your house burning to the ground.” | Like some of the solutions to NEPA problems involve replacing long studies & lawsuits with more bureaucrat civil servants empowered with discretion to make faster decisions on projects. So instead of a project needing to study every eventuality & have lawyers argue back/forth, a civil servant just reviews the project for if it goes against any regulations & communicates their decision. | https://www.noahpinion.blog/p/america-needs-a-bigger-better-bureaucracy | LikeLiked by 1 person | Δdocument.getElementById( ""ak_js_1"" ).setAttribute( ""value"", ( new Date() ).getTime() ); | Enter your email address to follow this blog and receive notifications of new posts by email. | Email Address: | Follow"
Dittemore's Law: observations on elite organization dysfunction,1 point,surprisetalk,youtube.com,https://caseyhandmer.wordpress.com/2025/01/17/dittemores-law/,"A quick note to formalize some observations on elite organization dysfunction. | The Space Mirror Memorial at Kennedy Space Center in Florida commemorates the 25 US astronauts who have died in flight. | Ron Dittemore is the retired former Space Shuttle program manager who was ultimately responsible for the series of decisions that resulted in the Columbia disaster, which killed seven of the lost 25 astronauts. | Throughout my career I’ve become increasingly obsessed with a particular and fairly obscure form of institutional failure. I haven’t seen much written about its general form, so I’m writing this post to collate my thoughts. | Dittemore’s Law states that “A team composed of sufficiently competent, motivated, well-resourced individuals will tend to produce a collective outcome that is diametrically opposed to the intended, individually desired outcome.” | In physics terms, it’s something like diamagnetism. | Dittemore’s Law is related to, but distinct from, several other wry observations about organizational and bureaucratic behavior. | Conquest’s Last Law states that “The simplest way to explain the behavior of any bureaucratic organization is to assume that it is controlled by a cabal of its enemies.” | While this can explain, for example, aspects of NASA’s dysfunction it is used more often to describe the emergent inefficiency so common in bureaucratic systems, rather than the startling anti-aligned competency seen in Dittemore’s Law. It seems, perhaps, that while the productivity of most teams converges monotonically to zero as bureaucracy increases, teams which are driven to producing actual outcomes through the provision of additional talent and resources can drive themselves beyond zero productivity into counter-productivity. | The Shirky Principle is the adage that “institutions will try to preserve the problem to which they are the solution”. | I think this can explain, for example, the proliferation of different mission assurance offices within NASA whose stated purpose was to, you know, assure the mission and ensure safety, but whose operational purpose was (and is) to carefully launder sketchy dispositions past objections from engineering management and “get to yes”. These various offices give rise to a process chart which is, by Conway’s Law, an exact mapping of the org chart and which is used to diffuse accountability. But, like Conquest’s Last Law, these processes are much more common and more usually explain organizational insanity and helplessness, rather than terrifying proficiency at breaking things. | Pournelle’s iron law of bureaucracy states that “In any bureaucracy, the people devoted to the benefit of the bureaucracy itself always get in control and those dedicated to the goals that the bureaucracy is supposed to accomplish have less and less influence, and sometimes are eliminated entirely.” | I think this is one mechanism by which Dittemore’s Law can occur. Within NASA there are plenty of dissenting voices, critical voices, skeptical voices, but the mission assurance apparatus overseen by Ron Dittemore and his colleagues in senior management, as described in various accident investigation reports, systematically sidelined information that conflicted with the desired outcome. But Dittemore’s Law can occur in relatively unbureaucratic places too, such as small team start up companies, failing marriages, and even with an individual unable to halt self-sabotage. | The coordination problem is a known hard problem, which has beset humanity at least since we first started to build pyramids. Getting large groups of people to work together on a shared collective enterprise is very unnatural and very difficult. Thousands of books have been written on the subject. The entire professional managerial class exists to try to solve this problem, with a track record best described, charitably, as a bloody stalemate. | Some examples | California is one of the wealthiest places in history, that easily attracts the smartest, most ambitious people into dozens of local industries. And yet, it is constantly beset by petty problems made enormous through the collective actions of numerous well-intentioned, well-resourced people. Public safety, policing, uncontrolled immigration, wildfire destruction, crumbling infrastructure, internal revenue challenges, housing scarcity, rampant inflation. Baumol cost disease is practically a local sport. | Well-intentioned environmental protection regulations such as NEPA, CEQA, and the CCA are routinely weaponized by a tiny handful of resourceful advocacy dogmatists to bring basic state functions operations to a grinding halt over decades, resulting in the ongoing collapse of agriculture, energy, and the major cities’ ability to house anyone who can’t afford to drop $5m on a quasi-derelict bungalow thrown up 100 years ago. | NASA is the place I first encountered this issue in a deliberate way. NASA is full of smart, well-intentioned, well-resourced people, and yet they can’t seem to stop finding ways to screw up basic processes. NASA can get 100 people on a project in a room, all with IQs north of 130, and everyone will agree exactly what the desired outcome is and how to get there, and then, like clockwork, the exact opposite thing will occur. The most obvious examples are the Apollo 1, Challenger and Columbia disasters which killed people, but there are numerous other near misses, poorly managed programs, and the ongoing spectacle of the failing SLS program trying to justify flying the Orion capsule, which is 20 years too late, $20b too expensive, and whose heat shield failed to operate properly – basic stuff. Even Mars Sample Return, a project in development since 1987, currently stands with no design reference architecture, budget, or schedule that maps to reality. | There are also numerous examples in private industry where professionals who should have known better managed to drive once-prosperous businesses straight into the ground. History is littered with the wreckage of former industrial titans that underestimated the impact of new technology and overestimated their ability to adapt. Blockbuster, Motorola, Kodak, Nokia, RIM, Xerox, Yahoo, IBM, Atari, Sears, Hitachi, Polaroid, Toshiba, HP, Palm, Sony, PanAm, Sega, Netscape, Compaq, Enron, GM, DeLorean, Nortel. In many cases, such as with Kodak and digital cameras, these powerful corporations even invented the technology that eventually destroyed them. It was not a surprise. Everyone saw it coming. But senior management failed to recognize that adaptation would require stepping beyond the accepted bounds of their traditional business practice. A team of well-resourced people with every reason to find some way to stay on top instead managed to snooker themselves into abject failure. | Since these are examples of people pouring resources into processes that ultimately had the opposite of the intended effect, they are probably also examples of Dittemore’s Law. | What are some other examples? | …and yet, from a standing start, they got to the Moon in 9 years. | Leon Skum hasn’t yet penetrated past low-Earth orbit. | But, yes, large groups of people in all states and nations do experience both efficiencies and inefficiencies. And comparing services to manufacturing is the most economically invalid thing a human can do that is not literally criminal. | LikeLike | Anthropologist James C. Scott in Seeing Like a State studies well intentioned bureaucratic schemes that have failed and tries to explain why they did. | https://en.wikipedia.org/wiki/Seeing_Like_a_State | LikeLike | My experience | LikeLike | The REASON for that is simple | It starts with “who gets promoted?” – two engineers of equal ability one concentrates on getting the job done – the other one concentrates on being promoted | Which one gets promoted first?? | In my decades in industry I have found that engineers and senior engineers are mostly “Task focussed” – but as you look up the ladder that changes – executives are 100% “ME focussed” | The senior people in any organisation are the ones who were best at the game of “Office Backstabber” | I will also add another reason actual competence is rare at senior levels | Assume that being an executive/CEO is a hard ball aching job | THEN the rational thing to do is to retire when you can afford to | Today’s massively overpaid executives are “insatiable” – reasonable people have earned $10million and retired | Insatiability is a form of mental illness | The old saying was – Pay peanuts Get monkeys | We should add – Pay millions Get loonies | LikeLike | Δdocument.getElementById( ""ak_js_1"" ).setAttribute( ""value"", ( new Date() ).getTime() ); | Enter your email address to follow this blog and receive notifications of new posts by email. | Email Address: | Follow"
Nice Videos on Analytic Number Theory,1 point,tinmandespot,useconsensus.io,https://www.youtube.com/playlist?list=PLbaA3qJlbE93DiTYMzl0XKnLn5df_QWqY,
Find out what people on Reddit think about your questions,2 points,Smujtab,apnews.com,https://useconsensus.io/,
A meteorite strike was captured on video by a Canadian home's doorbell camera,1 point,petethomas,phoronix.com,https://apnews.com/article/meteorite-strike-video-sound-canada-a93e11a07ead5c2336c066e072a0f970,"Copyright 2025 The Associated Press. All Rights Reserved. | The photo provided by the University of Alberta Meteorite Collection shows fragments collected from a meteorite which fell in front of a resident’s home in Marshfield, P.E.I., Canada, in July 2024. (University of Alberta Meteorite Collection/The Canadian Press via AP) | NEW YORK (AP) — A doorbell camera on a Canadian home captured rare video and sound of a meteorite striking Earth as it crashed into a couple’s walkway. | When Laura Kelly and her partner returned home after an evening walk in July, they were surprised to find their walkway littered with dust and strange debris, according to the Meteoritical Society, which posted the video with its report. | They checked their security camera and saw something slamming against their entranceway, producing a cloud of smoke and a crackle. | The pair reported what they found to the University of Alberta’s Meteorite Reporting System and the curator, Chris Herd, examined samples of the debris to confirm its interstellar origins. | Meteorites are bits of space rock that hit Earth after surviving a trip through its scorching atmosphere. About 48 tons (43,500 kilograms) of similar debris strikes Earth every day, according to NASA, but is much more likely to plunge into an ocean than onto someone’s front stoop. | The space rocks also streak the night sky as shooting stars during meteor showers which happen several times a year. | The footage is believed to be a first. While cameras have captured meteors streaking through the sky, it’s rare to capture the sound of a complete meteorite strike on video. | The space rock, officially registered Monday, was named Charlottetown after the city on Prince Edward Island in eastern Canada where it struck. | —- | The Associated Press Health and Science Department receives support from the Howard Hughes Medical Institute’s Science and Educational Media Group and the Robert Wood Johnson Foundation. The AP is solely responsible for all content. | Copyright 2025 The Associated Press. All Rights Reserved."
"PCI Express 7.0 Remains on Track for 2025, v0.7 Spec Published",1 point,LinuxBender,nytimes.com,https://www.phoronix.com/news/PCI-Express-7.0-v0.7-Spec,
"How TikTok Evaded a Ban Again and Again, Until Now",2 points,RyanShook,phoronix.com,https://www.nytimes.com/2025/01/17/technology/tiktok-ban-sale-supreme-court.html,"Looming TikTok Ban | Looming TikTok Ban | Looming TikTok Ban | Advertisement | Supported by | After a decisive loss at the Supreme Court, the app is set to be blocked in the U.S. starting Sunday, ending its streak of Houdini-like escapes. | By Sapna MaheshwariMadison Malone Kircher and David McCabe | In mid-2023, TikTok had just eluded an effort in Congress to ban the video app, the latest Houdini-like escape for the young tech company. For several years, during both Republican and Democratic administrations, lawmakers and officials had trained their sights on the app, saying its Chinese ownership posed a national security risk. | Inside TikTok, a small group of employees started formulating a plan to ensure that the regulatory threat would never reappear, three people with knowledge of the project said. The employees pitched a campaign of TV commercials, messages to users and other public advocacy to turn Washington’s attention elsewhere. They called it Project Achilles. | But TikTok’s leaders lost interest by the end of the year. Several, including Shou Chew, its chief executive, seemed to think the threat of a ban was no longer imminent, the people said. Project Achilles never became reality. | The misreading of the political winds could not have been greater. | Just a few months later, Congress overwhelmingly passed and President Biden signed a law that would ban TikTok unless the app’s owner, ByteDance, sold it to a non-Chinese company. On Friday, the Supreme Court upheld the law. TikTok is set to be removed from app stores on Sunday, when the law goes into effect. | The ban will end a remarkable eight-year roller-coaster ride for TikTok in the United States. The company wriggled its way out of political danger time and again. The threats to its very existence came so often, from so many directions, dealing with them became almost second nature for executives — perhaps to the point of complacency. | All the while, TikTok reached new heights of popularity and public influence. It boasts 170 million monthly U.S. users, giving the company confidence that those masses could help beat back whatever regulators aimed its way. Behind the scenes, TikTok conducted secretive negotiations with government officials and advertising blitzes aimed at rescuing it. | Thank you for your patience while we verify access. If you are in Reader mode please exit and log into your Times account, or subscribe for all of The Times. | Thank you for your patience while we verify access. | Already a subscriber? Log in. | Want all of The Times? Subscribe. | Advertisement"
Fedora 42 Boot Splash Screen Looks to Workaround GPU Drivers Taking Too Long,1 point,LinuxBender,symplecrm.com,https://www.phoronix.com/news/Fedora-42-Plymouth-Simple-DRM,
Show HN: SympleCRM – No-bloat CRM to simplify client and deal management,1 point,Charmizard,animeshgaitonde.medium.com,https://symplecrm.com/,
Breaking it down: The magic of multipart file uploads,1 point,thunderbong,twitter.com/dylanoa4,https://animeshgaitonde.medium.com/breaking-it-down-the-magic-of-multipart-file-uploads-98cb6fff65fe,"Sign up | Sign in | Sign up | Sign in | Member-only story | Animesh Gaitonde | Follow | -- | Share | Few years ago, I was asked to design a system that handles large file uploads in an interview. I was a mid-level software engineer and hadn’t tackled this problem before. | I had only used applications that handled file uploads like Gmail, Instagram etc. Since the user experience is seamless, I thought the problem would also be simple. | So, I went to the drawing board, sketched a couple of APIs, and boxes and told the interviewer that I am done. I found that it’s been only 10 minutes since the interview started. ⏲️ | But wait, aren’t tech design rounds supposed to last an hour ? I became skeptical and told the interviewer that I needed his help. | Luckily, I got an excellent interviewer who was not only strong technically but also gave me the right direction. 😇 | Although I wasn’t able to crack the interview but it gave me immense learnings and I was amazed at the elegant engineering behind large file uploads. | In this article, we will understand the internals of large file uploads. We will learn the high-level architecture, practical applications and different edge cases. | -- | -- | SDE-3/Tech Lead @ Amazon| ex-Airbnb | ex-Microsoft. Writes about Distributed Systems, Programming Languages & Tech Interviews | Help | Status | About | Careers | Press | Blog | Privacy | Terms | Text to speech | Teams"
Nick Cave on Cynicism vs. Hope,2 points,keepamovin,arstechnica.com,https://twitter.com/DylanoA4/status/1880307580867928146,
"RedNote may wall off ""TikTok refugees"" from Chinese users",3 points,ipython,krebsonsecurity.com,https://arstechnica.com/tech-policy/2025/01/rednote-may-wall-off-tiktok-refugees-to-prevent-us-influence-on-chinese-users/,"Rumors swirl that RedNote may segregate Chinese users as soon as next week. | Just a few days after more than 700 million new users flooded RedNote—which Time noted is ""the most apolitical social platform in China""—rumors began swirling that RedNote may soon start segregating American users and other foreign IPs from the app's Chinese users. | In the ""TikTokCringe"" subreddit, a video from a RedNote user with red eyes, presumably swollen from tears, suggested that Americans had possibly ruined the app for Chinese Americans who rely on RedNote to stay current on Chinese news and culture. | ""RedNote or Xiaohongshu released an update in the greater China region with the function to separate out foreign IPs, and there are now talks of moving all foreign IPs to a separate server and having a different IP for those who are in the greater China area,"" the Reddit poster said. ""I know through VPNs and other ways, people are still able to access the app, but essentially this is gonna kill the app for Chinese Americans who actually use the app to connect with Chinese content, Chinese language, Chinese culture."" | There has been no official announcement that such a change is coming, but Reddit commenters speculated that possibly the Chinese Communist Party (CCP) was requiring a change to stop American TikTokers from using the app to influence Chinese citizens. | ""CCP don't want American influence to spread to their citizens,"" a top commenter wrote. That prompted another commenter to respond, ""a bit of irony here,"" seemingly suggesting China walling off American TikTokers seems a lot like what the US government plans to do through the TikTok ban. But others pointed out that China has long blocked Western social media apps like Reddit and X to prevent such feared influence. ""If people couldn’t see this coming, then they have no idea what China’s politics are,"" one Redditor suggested in the thread. | On social media platforms like X (formerly Twitter), others discussed the potential update that could fence off Chinese users, with one post with 1.2 million views suggesting the update could come next week. | Unlike other popular Chinese apps that require a Chinese phone number to register as a user, like WeChat, RedNote has never had such a requirement and always operated one single version of its app, Reuters reported. So, a shift to segregating users would be a big change and could possibly take more time. | Whether TikTok will be banned in the US in three days is still up in the air. The Supreme Court has yet to announce its verdict on the constitutionality of a law requiring TikTok to either sell its US operations or shut down in the US. It's possible that the Supreme Court could ask for more time to deliberate, potentially delaying enforcement of the law as TikTok has requested until after Donald Trump takes office. | While the divest-or-sell law had bipartisan support when it passed last year, momentum has seemingly shifted this week. Senator Ed Markey (D-Mass.) has introduced a bill to extend the deadline ahead of a potential TikTok ban, and a top Trump adviser, Congressman Mike Waltz, has said that Trump plans to stop the ban and ""keep TikTok from going dark,"" the BBC reported. Even the Biden administration, whose justice department just finished arguing why the US needed to enforce the law to SCOTUS, ""is considering ways to keep TikTok available,"" sources told NBC News. | For RedNote and China, the app's sudden popularity as the US alternative to TikTok seems to have come as a surprise. A Beijing-based independent industry analyst, Liu Xingliang, told Reuters that RedNote was ""caught unprepared"" by the influx of users. | To keep restricted content off the app, RedNote allegedly has since been ""scrambling to find ways to moderate English-language content and build English-Chinese translation tools,"" two sources familiar with the company told Reuters. Time's reporting echoed that, noting that ""Red Note is urgently recruiting English content moderators [Chinese]"" became a trending topic Wednesday on the Chinese social media app Weibo. | Many analysts have suggested that Americans' fascination with RedNote will be short-lived. Liu told Reuters that ""American netizens are in a dissatisfied mood, and wanting to find another Chinese app to use is a catharsis of short-term emotions and a rebellious gesture."" But unfortunately, ""the experience on it is not very good for foreigners."" | On RedNote, Chinese users have warned Americans that China censors way more content than they're used to on TikTok. Analysts told The Washington Post that RedNote's ""focus on shopping and entertainment means it is often even more active in blocking content seen as too serious for the app’s target audience."" Chinese users warned Americans not to post about ""politics, religion, and drugs"" or risk ""account bans or legal repercussions, including jail time,"" Rest of World reported. Meanwhile, on Reddit, Americans received additional warnings about common RedNote scams and reasons accounts could be banned. But Rest of World noted that many so-called ""TikTok refugees"" migrating to RedNote do not ""seem to know, or care, about platform rules."" | Likely as a result of failing to read community guidelines, TechCrunch noted that many American RedNote users were quickly being banned. Some were kicked off for reasons they didn't totally understand because ""users are joining the app but are unable to read the Community Guidelines, which are written in Mandarin."" | Some bans were due to posting videos with a TikTok watermark, TechCrunch reported, while others were likely banned as suspected bots after several failed attempts to verify their US-based phone numbers. Several TikTokers have confirmed they're attempting to appeal their bans, TechCrunch reported, but no one's sure how easy it will be to reinstate their accounts. | Investors are closely watching to see if American users will stick with RedNote, though. On Thursday, Bloomberg reported that RedNote’s biggest shareholders were discussing selling their shares ""at a valuation of at least $20 billion."" Those negotiations could end, Bloomberg noted, if RedNote ends up being a flash in the pan or if TikTok remains available in the US. | RedNote isn't exactly a perfect alternative for TikTok. It was founded in 2013 by Mao Wenchao and Miranda Qu Fang as an Instagram-like knockoff where Chinese users and lifestyle influencers could share pictures and text posts. | If TikTokers politicize RedNote and influence Chinese users, some China-based RedNote users suggested, they could draw attention from the CCP, which the app has historically avoided. | RedNote's Chinese name, Xiaohongshu, literally translates to ""Little Red Book,"" which seems like a direct reference to a book of quotes from Chinese communist leader Mao Tse-tung. But The Washington Post reported that the app is designed to be apolitical and its co-founder, Mao, maintains that the name instead pays ""homage to the colors of his college,"" Stanford Business School, and his former employer, Bain Capital. | Ars Technica has been separating the signal from
          the noise for over 25 years. With our unique combination of
          technical savvy and wide-ranging interest in the technological arts
          and sciences, Ars is the trusted source in a sea of information. After
          all, you don’t need to know everything, only what’s important."
Chinese Innovations Spawn Wave of Toll Phishing via SMS,6 points,LinuxBender,youtube.com,https://krebsonsecurity.com/2025/01/chinese-innovations-spawn-wave-of-toll-phishing-via-sms/,"Residents across the United States are being inundated with text messages purporting to come from toll road operators like E-ZPass, warning that recipients face fines if a delinquent toll fee remains unpaid. Researchers say the surge in SMS spam coincides with new features added to a popular commercial phishing kit sold in China that makes it simple to set up convincing lures spoofing toll road operators in multiple U.S. states. | Last week, the Massachusetts Department of Transportation (MassDOT) warned residents to be on the lookout for a new SMS phishing or “smishing” scam targeting users of EZDriveMA, MassDOT’s all electronic tolling program. Those who fall for the scam are asked to provide payment card data, and eventually will be asked to supply a one-time password sent via SMS or a mobile authentication app. | Reports of similar SMS phishing attacks against customers of other U.S. state-run toll facilities surfaced around the same time as the MassDOT alert. People in Florida reported receiving SMS phishing that spoofed Sunpass, Florida’s prepaid toll program. | This phishing module for spoofing MassDOT’s EZDrive toll system was offered on Jan. 10, 2025 by a China-based SMS phishing service called “Lighthouse.” | In Texas, residents said they received text messages about unpaid tolls with the North Texas Toll Authority. Similar reports came from readers in California, Colorado, Connecticut, Minnesota, and Washington. This is by no means a comprehensive list. | A new module from the Lighthouse SMS phishing kit released Jan. 14 targets customers of the North Texas Toll Authority (NTTA). | In each case, the emergence of these SMS phishing attacks coincided with the release of new phishing kit capabilities that closely mimic these toll operator websites as they appear on mobile devices. Notably, none of the phishing pages will even load unless the website detects that the visitor is coming from a mobile device. | Ford Merrill works in security research at SecAlliance, a CSIS Security Group company. Merrill said the volume of SMS phishing attacks spoofing toll road operators skyrocketed after the New Year, when at least one Chinese cybercriminal group known for selling sophisticated SMS phishing kits began offering new phishing pages designed to spoof toll operators in various U.S. states. | According to Merrill, multiple China-based cybercriminals are selling distinct SMS-based phishing kits that each have hundreds or thousands of customers. The ultimate goal of these kits, he said, is to phish enough information from victims that their payment cards can be added to mobile wallets and used to buy goods at physical stores, online, or to launder money through shell companies. | A component of the Chinese SMS phishing kit Lighthouse made to target customers of The Toll Roads, which refers to several state routes through Orange County, Calif. | Merrill said the different purveyors of these SMS phishing tools traditionally have impersonated shipping companies, customs authorities, and even governments with tax refund lures and visa or immigration renewal scams targeting people who may be living abroad or new to a country. | “What we’re seeing with these tolls scams is just a continuation of the Chinese smishing groups rotating from package redelivery schemes to toll road scams,” Merrill said. “Every one of us by now is sick and tired of receiving these package smishing attacks, so now it’s a new twist on an existing scam.” | In October 2023, KrebsOnSecurity wrote about a massive uptick in SMS phishing scams targeting U.S. Postal Service customers. That story revealed the surge was tied to innovations introduced by “Chenlun,” a mainland China-based proprietor of a popular phishing kit and service. At the time, Chenlun had just introduced new phishing pages made to impersonate postal services in the United States and at least a dozen other countries. | SMS phishing kits are hardly new, but Merrill said Chinese smishing groups recently have introduced innovations in deliverability, by more seamlessly integrating their spam messages with Apple’s iMessage technology, and with RCS, the equivalent “rich text” messaging capability built into Android devices. | “While traditional smishing kits relied heavily on SMS for delivery, nowadays the actors make heavy use of iMessage and RCS because telecom operators can’t filter them and they likely have a higher success rate with these delivery channels,” he said. | It remains unclear how the phishers have selected their targets, or from where their data may be sourced. A notice from MassDOT cautions that “the targeted phone numbers seem to be chosen at random and are not uniquely associated with an account or usage of toll roads.” | Indeed, one reader shared on Mastodon yesterday that they’d received one of these SMS phishing attacks spoofing a local toll operator, when they didn’t even own a vehicle. | Targeted or not, these phishing websites are dangerous because they are operated dynamically in real-time by criminals. If you receive one of these messages, just ignore it or delete it, but please do not visit the phishing site. The FBI asks that before you bin the missives, consider filing a complaint with the agency’s Internet Crime Complaint Center (IC3), including the phone number where the text originated, and the website listed within the text. | This entry was posted on Thursday 16th of January 2025 04:18 PM | I’ve had at least 10 or so of these. I don’t own a car. They allege there’s a problem even from other states from where I live. I think for many who received these, it’s an easy and expensive trap to fall into. | Yup.  I live in Texas but have had a 508 area code for years.  The messages came from the Philippines but I guess I wasn’t supposed to notice that.  Massachusetts has always been a pretty long drive from here, like the Philippines is kind of a long swim from China. | This type of scam has been seen in Australia for many years.  If fact it is now so well known that most people are fully aware and delete the SMS immediately. | I’ve gotten several of these for Georgia’s PeachPass. A couple of months ago PeachPass actually put up a warning on their website about them. | I received one shortly after opening a snail mail from EZPass. Was “easy” to spot the scam. | I received several such text messages about three months ago here in Maryland. I told my wife (who also got these texts) not to act on it. I logged in to the regular web site without clicking on anything in the message. And Behold! there was an announcement from EZ Pass MD that there were impostor text messages and we should look out. | I received one of these SMS attacks last week. It was not too shabby, as far as phishing attacks go. I could see those attacks being good enough to lure several victims, unfortunately. | I got 4 of these, 2 on 1 day, 2 more on another day.
But…here’s what’s strange: they only showed up in messages on my Apple Watch Ultra 2 with cellular; they didn’t show up in messages on my iPad, or iPhone. | Further to my earlier comment he is a link to the Linkt Toll Scam in Melbourne, Australia that has been ongoing for  at least two years now.. https://www.linkt.com.au/help/security/latest-scams/melbourne
It shows several SMS messages, some purporting to be from the Toll Provider and some from a Debt Collection House.
Delete and do not open! | Further to my earlier comment here is a link to the Linkt Toll Scam in Melbourne, Australia that has been ongoing for  at least two years now.. https://www.linkt.com.au/help/security/latest-scams/melbourne
It shows several SMS messages, some purporting to be from the Toll Provider and some from a Debt Collection House.
Delete and do not open!  Be aware! | Does the Chinese government support these phishers? | Yes and no. By proxy they do but if it hurts their image they don’t. | Phishing is not a top tier cybercrime like say stealing corporate or state secrets, so fighting or defending against it only receives the attention/funding relative to the public interest. If Chinese society starts getting vocal about it, only then will the authorities act. They recently cracked down hard on activities operating out of Myanmar for this very reason and the message was; don’t target Chinese citizens. | Phishing and lower tier crimes of opportunity are generally only performed by organised crime groups or script kiddies, but groups operating out of Russia and China have been known for years to have support from state backed APT operatives, moonlighting for extra cash on the side. Unfortunately, like Russia, corruption exists and they are lowly paid, so their supervisors turn a blind eye. This is also why Western agencies have been able to identify and sanction so many individual operatives, simply because they get lazy and use the same tools and techniques regardless of who they are working for; an MO is like a fingerprint and leaves you vulnerable to discovery. | By comparison Western agencies, Five Eyes operatives especially, are bound by very strict rules of engagement for the very reasons stated above. They are highly educated and highly paid. Therefore moonlighting is not only frowned upon, but would be career ending and ostracised from the community forever. | I don’t trust text messages because there is no real way to authenticate them. | My rate of text messages went up in 2024 because of doctors.  It seems that doctors really love them for contacting patients. | I thought that I had found a good use for text messages when I found out that I could send e-mail to the telephone number ([cellnumber]@vtext.com for Verizon and [cellnumber]@txt.att.net for AT&T).  So the logical thing was to have my servers send text messages for various events to my cell phone so that I could keep up with what was happening even at times when I don’t have internet access. | I started with logins to the servers.  Even that was too much.  After about twenty minutes, the cell phone company stopped accepting messages.  I had figured realistically that I would be happy with maybe 40,000 messages a month to cover startups, shutdowns, logins, logouts, and some significant attempts to break in, but when it couldn’t barely handle five messages per hour, that was out of the question. | So I’m back to hating text messages. | Drag a few anchors over the middling kingdon’s cables.  “Chinese cybercriminal group” is redundant. | IN MN and have received a couple of these. Here you need to have a balance and CC on file to use the system so their story doesn’t wash with this process. | Here in Seattle area I got one of these from Good-To-Go, deleted, and several from US Post office-deleted, and yesterday one from Pay Pal, also ignored and deleted. Also two from bogus Amazon purchases. They got nothing but I did get my credit card re-issued. ( They were happy to do it after I explained all of the above) | Your email address will not be published. Required fields are marked * | Comment * | Name * | Email * | Website | Δdocument.getElementById( ""ak_js_1"" ).setAttribute( ""value"", ( new Date() ).getTime() ); | Mailing List | Search KrebsOnSecurity | Recent Posts | Story Categories | Why So Many Top Hackers Hail from Russia"
Family saves Palisades home from wildfire with a pool pump [video][3 mins],2 points,LinuxBender,,https://www.youtube.com/watch?v=yiYNWk1ut4k,
